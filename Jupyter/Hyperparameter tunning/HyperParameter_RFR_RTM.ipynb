{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HyperParameter_RFR_RTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4h63bK9iL2sg"
      },
      "source": [
        "Step 1: Import packages & stuff needed to generate the data for inversion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eGsrTISLu6k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "4fb9c7db-d452-4eaf-8b33-5157600641a8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#Perhaps this step can be skipped by saving directly to the workspace"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nuu-jvIMTcP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "57ccf3e6-eadf-4a9e-8648-a3f92b0bab94"
      },
      "source": [
        "#package instalation\n",
        "\n",
        "#Installing PROSAIL\n",
        "!pip install prosail\n",
        "\n",
        "#latin hypercube stuff\n",
        "#lets try to do a LHS\n",
        "!pip install lhsmdu\n",
        "\n",
        "#this package as a number of functions to deal with hyperspectral data\n",
        "#!pip install pysptools"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting prosail\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b5/52/d0c15ab469e8c82bc76a6b6cd614efbc60e43d09d5bacaa349170d229e91/prosail-2.0.5-py3-none-any.whl (149kB)\n",
            "\r\u001b[K     |██▏                             | 10kB 13.4MB/s eta 0:00:01\r\u001b[K     |████▍                           | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 40kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 153kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from prosail) (0.48.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from prosail) (1.18.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from prosail) (1.4.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from prosail) (3.6.4)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->prosail) (0.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->prosail) (46.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (1.12.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (8.3.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (1.8.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (19.3.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (1.4.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (0.7.1)\n",
            "Installing collected packages: prosail\n",
            "Successfully installed prosail-2.0.5\n",
            "Collecting lhsmdu\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/f0/e714a4dae734bcd7228a09d74fff7dc5857dc3311cd72a3e07b09c85d088/lhsmdu-0.1-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lhsmdu) (1.18.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lhsmdu) (1.4.1)\n",
            "Installing collected packages: lhsmdu\n",
            "Successfully installed lhsmdu-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWA_vt4HOyop",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "ec2cc6ff-1303-4d91-e4d6-59cd00593f6a"
      },
      "source": [
        "!pip install hyperopt "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (0.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.12.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt) (2.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from hyperopt) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.18.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.4.1)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt) (3.10.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt) (0.16.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt) (4.4.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfOcve1fMYBo"
      },
      "source": [
        "Packages for RandomForest and other auxiliary stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7SW2mrcaKA0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8e000731-17f7-42bb-ccae-b1c49db35132"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "for i in range(50, 1001, 50):\n",
        "    print(i, end=', ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzNjF9ldMXTM"
      },
      "source": [
        "#the beutiful R like data frame\n",
        "import pandas as pd\n",
        "\n",
        "#the famous numpy \n",
        "import numpy as np\n",
        "\n",
        "#PROSPECT+SAIL Radiative transfer mode package\n",
        "import prosail\n",
        "\n",
        "#Sampling design package\n",
        "import lhsmdu\n",
        "\n",
        "#a few more stuff for random\n",
        "import random as rdm\n",
        "import math\n",
        "\n",
        "\n",
        "# import the regressor for random forests\n",
        "from sklearn.ensemble import RandomForestRegressor \n",
        "\n",
        "from sklearn import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdxUW6drNjZc"
      },
      "source": [
        "Bunch of functions needed for generating data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hk-0MPrdNrGY"
      },
      "source": [
        "def custom_prosail(cab,cw,cm,lai):\n",
        "  import prosail\n",
        "  #default parameters\n",
        "  n= 1.\n",
        "  car=10.\n",
        "  cbrown=0.01\n",
        "  typelidf=1 #this is the default option\n",
        "  lidfa = -1 #leaf angle distribution parameter a and b\n",
        "  lidfb=-0.15\n",
        "  hspot= 0.01 #hotspot parameters - got this from R package https://www.rdocumentation.org/packages/hsdar/versions/0.4.1/topics/PROSAIL\n",
        "  #sun and viewing angle\n",
        "  tts=30. #observation and solar position parameters\n",
        "  tto=10. \n",
        "  psi=0.\n",
        "  #for now i put them by hand but they should be an input of a custom function\n",
        "  #tts=sol_zen #solar zenith angle\n",
        "  #tto=inc_zen #sensor zenith angle\n",
        "  #psi=raa\n",
        "  rho_out = prosail.run_prosail(n,\n",
        "                                 cab,\n",
        "                                 car,\n",
        "                                 cbrown,\n",
        "                                 cw,\n",
        "                                 cm,\n",
        "                                 lai,\n",
        "                                 lidfa,\n",
        "                                 hspot,\n",
        "                                 tts,tto,psi,\n",
        "                                 typelidf, lidfb,\n",
        "                                 prospect_version=\"D\",\n",
        "                                 factor='SDR', \n",
        "                                 rsoil=.5, psoil=.5)\n",
        "  return(rho_out)\n",
        "\n",
        "def Prosail2S2(path2csv,spectra_input):\n",
        "  #importing pandas\n",
        "  import pandas as pd\n",
        "  import numpy\n",
        "  import numpy as np\n",
        "  #upload a S2_Response.csv from https://earth.esa.int/web/sentinel/user-guides/sentinel-2-msi/document-library/-/asset_publisher/Wk0TKajiISaR/content/sentinel-2a-spectral-responses\n",
        "\n",
        "  s2_table = pd.read_csv(path2csv,sep=\";\",decimal=\",\") #check if this is proper, regarding the sep and dec\n",
        "  #chekc which row you are actually extracting\n",
        "\n",
        "  s2_table_sel = s2_table[s2_table['SR_WL'].between(400,2500)] #selects all values between 400 and 2500\n",
        "  spectra_input_df = pd.DataFrame(data=spectra_input,columns=[\"rho\"],index=s2_table_sel.index) #transforms the input array into a pandas df with the column name rho and row.index = to the original input table\n",
        "\n",
        "  \n",
        "  rho_s2 = s2_table_sel.multiply(spectra_input_df['rho'],axis=\"index\") #calculates the numerator\n",
        "  w_band_sum = s2_table_sel.sum(axis=0,skipna = True) #calculates the denominator\n",
        "\n",
        "  output = (rho_s2.sum(axis=0)/w_band_sum).rename_axis(\"ID\").values #runs the weighted mean and converts the output to a numpy array\n",
        "\n",
        "  return output[1:] #removes the first value because it represents the wavelength column\n",
        "\n",
        "#please LOAD THTE FILE NOW\n",
        "filepath=\"/content/drive/My Drive/S2_Response.csv\"\n",
        "#filepath=\"/content/S2_Responses_S2B.csv\"\n",
        "#filepath=\"/content/drive/My Drive/S2_Response.csv\"\n",
        "\n",
        "\n",
        "def Gen_spectra_data(traits):\n",
        "  k = 1\n",
        "  #pd_train_traits=traits\n",
        "  #print(range(len(traits)))\n",
        "  for i in range(len(traits)):\n",
        "    #n_t = pd_train_traits[\"n\"][i]\n",
        "    cab_t = traits[\"cab\"][i]\n",
        "    #car_t = pd_train_traits[\"car\"][i]\n",
        "    #cbrown_t = pd_train_traits[\"cbrown\"][i]\n",
        "    cw_t = traits[\"cw\"][i]\n",
        "    cm_t = traits[\"cm\"][i]\n",
        "    lai_t = traits[\"lai\"][i]\n",
        "\n",
        "    if k == 1:\n",
        "      tr_rho_s = custom_prosail(cab_t,cw_t,cm_t,lai_t)\n",
        "      tr_rho_s = Prosail2S2(filepath,tr_rho_s)\n",
        "      #plt.plot ( x, tr_rho_s, ':', label=\"Training prosail\")\n",
        "      #plt.legend(loc='best')\n",
        "      \n",
        "    if k > 1:\n",
        "      tr_rho_t = custom_prosail(cab_t,cw_t,cm_t,lai_t)\n",
        "      tr_rho_t = Prosail2S2(filepath,tr_rho_t)\n",
        "      tr_rho_s = np.vstack((tr_rho_s,tr_rho_t))\n",
        "      #plt.plot ( x, tr_rho_t, ':')\n",
        "\n",
        "    k = k+1\n",
        "\n",
        "\n",
        "  rho_samples=tr_rho_s\n",
        "\n",
        "\n",
        "  return rho_samples\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Function to store the outputs into a table\n",
        "column_names=[\"Model\",\n",
        "              \"NSamples\",\n",
        "              \"OutOfBag\",\n",
        "              \"KFold_tr_samples\",\n",
        "              \"KFold_vl_samples\",\n",
        "              \"Variable\",\n",
        "              \"Fold_nr\",\n",
        "              \"ExplVar\",\n",
        "              \"Max_err\",\n",
        "              \"Mean_abs_Err\",\n",
        "              \"Mean_sqr_err\",\n",
        "              #\"Mean_sqr_lg_err\",\n",
        "              \"Median_abs_err\",\n",
        "              \"r2\",\n",
        "              \"MAPE\",\n",
        "              \"ModelName\"]\n",
        "              #\"Mean_poiss_dev\",\n",
        "              #\"Mean_gamma_dev\"]\n",
        "              #\"Mean_tweed_dev\"]\n",
        "\n",
        "#mape is not existant in the package so we have to create it:\n",
        "#https://stats.stackexchange.com/questions/58391/mean-absolute-percentage-error-mape-in-scikit-learn\n",
        "#from sklearn.utils import check_array\n",
        "def mean_absolute_percentage_error(y_true, y_pred): \n",
        "\n",
        "    ## Note: does not handle mix 1d representation\n",
        "    #if _is_1d(y_true): \n",
        "    #    y_true, y_pred = _check_1d_array(y_true, y_pred)\n",
        "\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "  #Here, the file is used for saving\n",
        "  #creating a df to receive the data\n",
        "\n",
        "\n",
        "def calc_metrics(MDL,Samples,oob_samples,kf_tr,kf_vl,Variable,Fold,Ref,Pred,Modelname):\n",
        "\n",
        "  out_list = {\"Model\":MDL,\n",
        "              \"NSamples\":Samples,\n",
        "              \"OutOfBag\":oob_samples,\n",
        "              \"KFold_tr_samples\":kf_tr,\n",
        "              \"KFold_vl_samples\":kf_vl,\n",
        "              \"Variable\":Variable,\n",
        "              \"Fold_nr\":Fold,\n",
        "              \"ExplVar\": metrics.explained_variance_score(Ref, Pred),\n",
        "              \"Max_err\": metrics.max_error(Ref, Pred),\n",
        "              \"Mean_abs_Err\": metrics.mean_absolute_error(Ref, Pred),\n",
        "              \"Mean_sqr_err\": metrics.mean_squared_error(Ref, Pred),\n",
        "              \"Median_abs_err\" : metrics.median_absolute_error(Ref, Pred),\n",
        "              \"r2\": metrics.r2_score(Ref, Pred),\n",
        "              \"MAPE\": mean_absolute_percentage_error(Ref, Pred),\n",
        "              \"ModelName\":Modelname}\n",
        "\n",
        "\n",
        "  return out_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFeTJUgbN2k8"
      },
      "source": [
        "Hyperparameter tuning section\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbKvtfA0OumM"
      },
      "source": [
        "# define an objective function\n",
        "\n",
        "# These are a bunch of methods of te hyperopt package to generate the search space\n",
        "from hyperopt import hp\n",
        "\n",
        "# these are the minimizing function (fmin), Tree-parzen estimator method, an evaluation function, trial method and status indicators\n",
        "from hyperopt import fmin, tpe, space_eval, Trials, STATUS_OK, STATUS_FAIL\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FxnCB3_PY5B"
      },
      "source": [
        "Step 2: Generate the data that is going to be used for training\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K-075akPeZ6"
      },
      "source": [
        "#first the trait-space\n",
        "\n",
        "n_traits=4 #I will test on 4 varying traits: cab, car, cw,cm,lai\n",
        "\n",
        "#generating a LHS hypercube (it uses a 0 to 1 interval that can be used as a multiplier against the different traits)\n",
        "np.random.seed(0)\n",
        "\n",
        "\n",
        "#the values here are 1 less than the max so i can add a value later to it\n",
        "#max_n=1 \n",
        "max_cab=121. #add 1\n",
        "#max_car=44. #add 1\n",
        "#max_cbrown= 9.99 #add 0.01\n",
        "max_cw=0.008 #add 0.001 \n",
        "max_cm=0.008 #0.001\n",
        "max_lai = 9.9 #add 0.1\n",
        "\n",
        "min_cab = 10.\n",
        "min_cw = 0.002\n",
        "min_cm = 0.002\n",
        "min_lai = .5\n",
        "\n",
        "#set the number of samples used for the optimization\n",
        "n_samples = 2000\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5D7pk_9TPyiX"
      },
      "source": [
        "#this is for the training dataset\n",
        "df_metrics = pd.DataFrame(columns=column_names)\n",
        "#this is for the out of bag\n",
        "df_metrics_valid = pd.DataFrame(columns=column_names)\n",
        "\n",
        "#a path to store the models \n",
        "#path2folder = \"/content/drive/My Drive/DSRP_Lunch_outputs/Models/\"\n",
        "path2folder = \"/content/drive/My Drive/RTM_Inversion/HyperParameterTunning/RandF/\"\n",
        "\n",
        "#generating trait space and data\n",
        "LHS_train = lhsmdu.createRandomStandardUniformMatrix(n_traits,n_samples)\n",
        "pd_trait = pd.DataFrame.transpose(pd.DataFrame(LHS_train))\n",
        "pd_trait.columns = [\"cab\",\"cw\",\"cm\",\"lai\"]\n",
        "\n",
        "pd_trait[\"cab\"]=pd_trait[\"cab\"]*max_cab+min_cab\n",
        "pd_trait[\"cw\"] =pd_trait[\"cw\"] *max_cw+min_cw\n",
        "pd_trait[\"cm\"] =pd_trait[\"cm\"] *max_cm+min_cw\n",
        "pd_trait[\"lai\"]=pd_trait[\"lai\"]*max_lai+min_lai\n",
        "\n",
        "#this is the same as above but on numpy format\n",
        "np_trait = pd_trait.iloc[:,:].values\n",
        "\n",
        "#now we first generate the date in hyperspectral, convolute it to S2 while iterating through the entire given trait space\n",
        "np_spect = Gen_spectra_data(pd_trait)[:,[1,2,3,4,5,6,8,11,12]]\n",
        "\n",
        "#now we remove 10% of the data for out-of-bag validation of the final model while the cross-validation happens within the hyperparemeter tunning\n",
        "index = list(range(len(np_spect)))\n",
        "index10 = rdm.sample(index,math.ceil(len(index)*.1))\n",
        "index90 = [x for x in index if x not in index10]\n",
        "\n",
        "#selecting the groups for training (tr) and validation (vl)\n",
        "tr_trait_df = np_trait[index90,]\n",
        "tr_spect_df = np_spect[index90,]\n",
        "\n",
        "vl_trait_df = np_trait[index10,]\n",
        "vl_spect_df = np_spect[index10,]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLbGumF3SaOF"
      },
      "source": [
        "Definining the optimization function and the search space\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28j686EqSZiL"
      },
      "source": [
        "#objective function\n",
        "\n",
        "#first we define the function\n",
        "N_FOLDS = 10\n",
        "MAX_EVALS = 200\n",
        "\n",
        "#here we can just fetch the data so we keep everything in one place\n",
        "train_labels = tr_trait_df = np_trait[index90,]\n",
        "train_features = tr_spect_df = np_spect[index90,]\n",
        "\n",
        "\n",
        "def objective(params, n_folds = N_FOLDS):\n",
        "    \"\"\"Objective function for Random forest Hyperparameter Tuning\"\"\"\n",
        "\n",
        "    # Perform n_fold cross validation with hyperparameters\n",
        "    # Use early stopping and evaluate based on ROC AUC\n",
        "    rf = RandomForestRegressor(**params, random_state = 42,bootstrap=True,oob_score=True)\n",
        "\n",
        "    #clf = LogisticRegression(**params,random_state=0,verbose =0)\n",
        "    scores = cross_val_score(rf, X=train_features, y=train_labels, cv=10, scoring='neg_mean_absolute_error') #thi is the simple Mean abs\n",
        "\n",
        "    # Extract the best score\n",
        "    best_score = np.mean(abs(scores)) #mean value\n",
        "    # if the the result is miniming\n",
        "\n",
        "    # Loss must be minimized\n",
        "    #loss = 1 - best_score \n",
        "    loss = best_score \n",
        "\n",
        "    # Dictionary with information for evaluation\n",
        "    return {'loss': loss, 'params': params, 'status': STATUS_OK}\n",
        "\n",
        "#distribution functions for the parameter sampling are here:\n",
        "#https://github.com/hyperopt/hyperopt/wiki/FMin\n",
        "\n",
        "space = {\n",
        "    'n_estimators': hp.choice('n_estimators', range(50, 1001, 50)), #to avoid errors (e.g. to few trees)\n",
        "    'min_samples_split' : hp.uniform('min_samples_split', 0,.5), #these must be a fractio up to 50% of the data it seems\n",
        "    'min_samples_leaf' : hp.uniform('min_samples_leaf', 0,.5)\n",
        "}\n",
        "\n",
        "# for i in range(50, 1001, 50):\n",
        "#     print(i, end=', ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVIydEOQSsHW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8125d52d-1595-429d-ea14-901e36fb10ed"
      },
      "source": [
        "#this loop is the actual testing\n",
        "bayes_trials = Trials()\n",
        "\n",
        "best_rfr = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = MAX_EVALS, trials = bayes_trials,verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [1:04:32<00:00, 19.36s/it, best loss: 0.31946225928160776]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bnbvVAdXLkj"
      },
      "source": [
        "Quick testing to check for improvements "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ce0C3dSvXKzO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1906ea4e-eca1-48d7-d717-3d63363745fb"
      },
      "source": [
        "#beware, pointers effect is happening here\n",
        "print(best_rfr)\n",
        "#updating the dictionary\n",
        "#best_rfr_new = best_rfr\n",
        "#best_rfr_new.update(n_estimators=range(50, 1001, 50)[best_rfr['n_estimators']])\n",
        "print(range(50, 1001, 50)[15])\n",
        "#it updates the pointer so it can only be run once"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'min_samples_leaf': 0.0007218586390063501, 'min_samples_split': 0.0006729906646919075, 'n_estimators': 17}\n",
            "800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEMnUQywo9I0"
      },
      "source": [
        "Run 01:\n",
        "\n",
        "\n",
        "{'min_samples_leaf': 0.0007027343011890524, 'min_samples_split': 0.004091222247067641, 'n_estimators': 800} \n",
        "\n",
        "run 02\n",
        "\n",
        "{'min_samples_leaf': 0.0014275823590012802, 'min_samples_split': 0.0014118318943800098, 'n_estimators': 400}\n",
        "\n",
        "\n",
        "run 03\n",
        "{'min_samples_leaf': 0.0007218586390063501, 'min_samples_split': 0.0006729906646919075, 'n_estimators': 17}\n",
        "900\n",
        "\n"
      ]
    }
  ]
}