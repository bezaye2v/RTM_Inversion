{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HyperParameter_GPR_RTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVlg9bx3PNlC"
      },
      "source": [
        "\n",
        "Step 1: Import packages & stuff needed to generate the data for inversion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvn2K9tbEgd-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "620726d5-4051-4538-a16f-d9e040118edb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#Perhaps this step can be skipped by saving directly to the workspace"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7Kj7sUNPL3G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "169efe5c-554c-47e0-d89c-df5c0fec0295"
      },
      "source": [
        "#package instalation\n",
        "\n",
        "#Installing PROSAIL\n",
        "!pip install prosail\n",
        "\n",
        "#latin hypercube stuff\n",
        "#lets try to do a LHS\n",
        "!pip install lhsmdu\n",
        "\n",
        "#this package as a number of functions to deal with hyperspectral data\n",
        "#!pip install pysptools"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting prosail\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b5/52/d0c15ab469e8c82bc76a6b6cd614efbc60e43d09d5bacaa349170d229e91/prosail-2.0.5-py3-none-any.whl (149kB)\n",
            "\r\u001b[K     |██▏                             | 10kB 15.9MB/s eta 0:00:01\r\u001b[K     |████▍                           | 20kB 1.5MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 30kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 40kB 1.9MB/s eta 0:00:01\r\u001b[K     |███████████                     | 51kB 1.8MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 61kB 1.9MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 71kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 81kB 2.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 92kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 102kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 112kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 122kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 133kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 143kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 153kB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from prosail) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from prosail) (1.18.4)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from prosail) (0.48.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from prosail) (3.6.4)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->prosail) (0.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->prosail) (46.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (8.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (1.12.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (1.8.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (19.3.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (1.4.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (0.7.1)\n",
            "Installing collected packages: prosail\n",
            "Successfully installed prosail-2.0.5\n",
            "Collecting lhsmdu\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/f0/e714a4dae734bcd7228a09d74fff7dc5857dc3311cd72a3e07b09c85d088/lhsmdu-0.1-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lhsmdu) (1.18.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lhsmdu) (1.4.1)\n",
            "Installing collected packages: lhsmdu\n",
            "Successfully installed lhsmdu-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UV3WvQ8PeU2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "5cfa4d53-6ce1-42ea-f7de-cea892d5f60b"
      },
      "source": [
        "!pip install hyperopt "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (0.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from hyperopt) (4.41.1)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt) (3.10.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt) (2.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt) (0.16.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.12.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt) (4.4.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffgkb6EjPgJq"
      },
      "source": [
        "Packages for RandomForest and other auxiliary stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jf01gPABPjd2"
      },
      "source": [
        "#the beutiful R like data frame\n",
        "import pandas as pd\n",
        "\n",
        "#the famous numpy \n",
        "import numpy as np\n",
        "\n",
        "#PROSPECT+SAIL Radiative transfer mode package\n",
        "import prosail\n",
        "\n",
        "#Sampling design package\n",
        "import lhsmdu\n",
        "\n",
        "#a few more stuff for random\n",
        "import random as rdm\n",
        "import math\n",
        "\n",
        "\n",
        "# import the regressor for gaussian processes and also the pre-set group of kernels\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF,DotProduct,Matern,ExpSineSquared,RationalQuadratic\n",
        "\n",
        "\n",
        "from sklearn import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf0CAwQePmC2"
      },
      "source": [
        "Bunch of functions needed for generating data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTAkzLnPPmiG"
      },
      "source": [
        "def custom_prosail(cab,cw,cm,lai):\n",
        "  import prosail\n",
        "  #default parameters\n",
        "  n= 1.\n",
        "  car=10.\n",
        "  cbrown=0.01\n",
        "  typelidf=1 #this is the default option\n",
        "  lidfa = -1 #leaf angle distribution parameter a and b\n",
        "  lidfb=-0.15\n",
        "  hspot= 0.01 #hotspot parameters - got this from R package https://www.rdocumentation.org/packages/hsdar/versions/0.4.1/topics/PROSAIL\n",
        "  #sun and viewing angle\n",
        "  tts=30. #observation and solar position parameters\n",
        "  tto=10. \n",
        "  psi=0.\n",
        "  #for now i put them by hand but they should be an input of a custom function\n",
        "  #tts=sol_zen #solar zenith angle\n",
        "  #tto=inc_zen #sensor zenith angle\n",
        "  #psi=raa\n",
        "  rho_out = prosail.run_prosail(n,\n",
        "                                 cab,\n",
        "                                 car,\n",
        "                                 cbrown,\n",
        "                                 cw,\n",
        "                                 cm,\n",
        "                                 lai,\n",
        "                                 lidfa,\n",
        "                                 hspot,\n",
        "                                 tts,tto,psi,\n",
        "                                 typelidf, lidfb,\n",
        "                                 prospect_version=\"D\",\n",
        "                                 factor='SDR', \n",
        "                                 rsoil=.5, psoil=.5)\n",
        "  return(rho_out)\n",
        "\n",
        "def Prosail2S2(path2csv,spectra_input):\n",
        "  #importing pandas\n",
        "  import pandas as pd\n",
        "  import numpy\n",
        "  import numpy as np\n",
        "  #upload a S2_Response.csv from https://earth.esa.int/web/sentinel/user-guides/sentinel-2-msi/document-library/-/asset_publisher/Wk0TKajiISaR/content/sentinel-2a-spectral-responses\n",
        "\n",
        "  s2_table = pd.read_csv(path2csv,sep=\";\",decimal=\",\") #check if this is proper, regarding the sep and dec\n",
        "  #chekc which row you are actually extracting\n",
        "\n",
        "  s2_table_sel = s2_table[s2_table['SR_WL'].between(400,2500)] #selects all values between 400 and 2500\n",
        "  spectra_input_df = pd.DataFrame(data=spectra_input,columns=[\"rho\"],index=s2_table_sel.index) #transforms the input array into a pandas df with the column name rho and row.index = to the original input table\n",
        "\n",
        "  \n",
        "  rho_s2 = s2_table_sel.multiply(spectra_input_df['rho'],axis=\"index\") #calculates the numerator\n",
        "  w_band_sum = s2_table_sel.sum(axis=0,skipna = True) #calculates the denominator\n",
        "\n",
        "  output = (rho_s2.sum(axis=0)/w_band_sum).rename_axis(\"ID\").values #runs the weighted mean and converts the output to a numpy array\n",
        "\n",
        "  return output[1:] #removes the first value because it represents the wavelength column\n",
        "\n",
        "#please LOAD THTE FILE NOW\n",
        "filepath=\"/content/drive/My Drive/S2_Response.csv\"\n",
        "#filepath=\"/content/S2_Responses_S2B.csv\"\n",
        "#filepath=\"/content/drive/My Drive/S2_Response.csv\"\n",
        "\n",
        "\n",
        "def Gen_spectra_data(traits):\n",
        "  k = 1\n",
        "  #pd_train_traits=traits\n",
        "  #print(range(len(traits)))\n",
        "  for i in range(len(traits)):\n",
        "    #n_t = pd_train_traits[\"n\"][i]\n",
        "    cab_t = traits[\"cab\"][i]\n",
        "    #car_t = pd_train_traits[\"car\"][i]\n",
        "    #cbrown_t = pd_train_traits[\"cbrown\"][i]\n",
        "    cw_t = traits[\"cw\"][i]\n",
        "    cm_t = traits[\"cm\"][i]\n",
        "    lai_t = traits[\"lai\"][i]\n",
        "\n",
        "    if k == 1:\n",
        "      tr_rho_s = custom_prosail(cab_t,cw_t,cm_t,lai_t)\n",
        "      tr_rho_s = Prosail2S2(filepath,tr_rho_s)\n",
        "      #plt.plot ( x, tr_rho_s, ':', label=\"Training prosail\")\n",
        "      #plt.legend(loc='best')\n",
        "      \n",
        "    if k > 1:\n",
        "      tr_rho_t = custom_prosail(cab_t,cw_t,cm_t,lai_t)\n",
        "      tr_rho_t = Prosail2S2(filepath,tr_rho_t)\n",
        "      tr_rho_s = np.vstack((tr_rho_s,tr_rho_t))\n",
        "      #plt.plot ( x, tr_rho_t, ':')\n",
        "\n",
        "    k = k+1\n",
        "\n",
        "\n",
        "  rho_samples=tr_rho_s\n",
        "\n",
        "\n",
        "  return rho_samples\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Function to store the outputs into a table\n",
        "column_names=[\"Model\",\n",
        "              \"NSamples\",\n",
        "              \"OutOfBag\",\n",
        "              \"KFold_tr_samples\",\n",
        "              \"KFold_vl_samples\",\n",
        "              \"Variable\",\n",
        "              \"Fold_nr\",\n",
        "              \"ExplVar\",\n",
        "              \"Max_err\",\n",
        "              \"Mean_abs_Err\",\n",
        "              \"Mean_sqr_err\",\n",
        "              #\"Mean_sqr_lg_err\",\n",
        "              \"Median_abs_err\",\n",
        "              \"r2\",\n",
        "              \"MAPE\",\n",
        "              \"ModelName\"]\n",
        "              #\"Mean_poiss_dev\",\n",
        "              #\"Mean_gamma_dev\"]\n",
        "              #\"Mean_tweed_dev\"]\n",
        "\n",
        "#mape is not existant in the package so we have to create it:\n",
        "#https://stats.stackexchange.com/questions/58391/mean-absolute-percentage-error-mape-in-scikit-learn\n",
        "#from sklearn.utils import check_array\n",
        "def mean_absolute_percentage_error(y_true, y_pred): \n",
        "\n",
        "    ## Note: does not handle mix 1d representation\n",
        "    #if _is_1d(y_true): \n",
        "    #    y_true, y_pred = _check_1d_array(y_true, y_pred)\n",
        "\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "  #Here, the file is used for saving\n",
        "  #creating a df to receive the data\n",
        "\n",
        "\n",
        "def calc_metrics(MDL,Samples,oob_samples,kf_tr,kf_vl,Variable,Fold,Ref,Pred,Modelname):\n",
        "\n",
        "  out_list = {\"Model\":MDL,\n",
        "              \"NSamples\":Samples,\n",
        "              \"OutOfBag\":oob_samples,\n",
        "              \"KFold_tr_samples\":kf_tr,\n",
        "              \"KFold_vl_samples\":kf_vl,\n",
        "              \"Variable\":Variable,\n",
        "              \"Fold_nr\":Fold,\n",
        "              \"ExplVar\": metrics.explained_variance_score(Ref, Pred),\n",
        "              \"Max_err\": metrics.max_error(Ref, Pred),\n",
        "              \"Mean_abs_Err\": metrics.mean_absolute_error(Ref, Pred),\n",
        "              \"Mean_sqr_err\": metrics.mean_squared_error(Ref, Pred),\n",
        "              \"Median_abs_err\" : metrics.median_absolute_error(Ref, Pred),\n",
        "              \"r2\": metrics.r2_score(Ref, Pred),\n",
        "              \"MAPE\": mean_absolute_percentage_error(Ref, Pred),\n",
        "              \"ModelName\":Modelname}\n",
        "\n",
        "\n",
        "  return out_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH5x-3a2PpZn"
      },
      "source": [
        "Hyperparameter tuning section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ci2Q04P5Ptt_"
      },
      "source": [
        "# define an objective function\n",
        "\n",
        "# These are a bunch of methods of te hyperopt package to generate the search space\n",
        "from hyperopt import hp\n",
        "\n",
        "# these are the minimizing function (fmin), Tree-parzen estimator method, an evaluation function, trial method and status indicators\n",
        "from hyperopt import fmin, tpe, space_eval, Trials, STATUS_OK, STATUS_FAIL\n",
        "from sklearn.model_selection import cross_val_score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zOs0-UNPv93"
      },
      "source": [
        "Step 2: Generate the data that is going to be used for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-Cn1W_aPw5v"
      },
      "source": [
        "#first the trait-space\n",
        "\n",
        "n_traits=4 #I will test on 4 varying traits: cab, car, cw,cm,lai\n",
        "\n",
        "#generating a LHS hypercube (it uses a 0 to 1 interval that can be used as a multiplier against the different traits)\n",
        "np.random.seed(0)\n",
        "\n",
        "\n",
        "#the values here are 1 less than the max so i can add a value later to it\n",
        "#max_n=1 \n",
        "max_cab=121. #add 1\n",
        "#max_car=44. #add 1\n",
        "#max_cbrown= 9.99 #add 0.01\n",
        "max_cw=0.008 #add 0.001 \n",
        "max_cm=0.008 #0.001\n",
        "max_lai = 9.9 #add 0.1\n",
        "\n",
        "min_cab = 10.\n",
        "min_cw = 0.002\n",
        "min_cm = 0.002\n",
        "min_lai = .5\n",
        "\n",
        "#set the number of samples used for the optimization\n",
        "n_samples = 1500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUsffTpDP26P"
      },
      "source": [
        "#this is for the training dataset\n",
        "df_metrics = pd.DataFrame(columns=column_names)\n",
        "#this is for the out of bag\n",
        "df_metrics_valid = pd.DataFrame(columns=column_names)\n",
        "\n",
        "#a path to store the models \n",
        "#path2folder = \"/content/drive/My Drive/DSRP_Lunch_outputs/Models/\"\n",
        "path2folder = \"/content/drive/My Drive/RTM_Inversion/HyperParameterTunning/RandF/\"\n",
        "\n",
        "#generating trait space and data\n",
        "LHS_train = lhsmdu.createRandomStandardUniformMatrix(n_traits,n_samples)\n",
        "pd_trait = pd.DataFrame.transpose(pd.DataFrame(LHS_train))\n",
        "pd_trait.columns = [\"cab\",\"cw\",\"cm\",\"lai\"]\n",
        "\n",
        "pd_trait[\"cab\"]=pd_trait[\"cab\"]*max_cab+min_cab\n",
        "pd_trait[\"cw\"] =pd_trait[\"cw\"] *max_cw+min_cw\n",
        "pd_trait[\"cm\"] =pd_trait[\"cm\"] *max_cm+min_cw\n",
        "pd_trait[\"lai\"]=pd_trait[\"lai\"]*max_lai+min_lai\n",
        "\n",
        "#this is the same as above but on numpy format\n",
        "np_trait = pd_trait.iloc[:,:].values\n",
        "\n",
        "#now we first generate the date in hyperspectral, convolute it to S2 while iterating through the entire given trait space\n",
        "np_spect = Gen_spectra_data(pd_trait)[:,[1,2,3,4,5,6,8,11,12]]\n",
        "\n",
        "#now we remove 10% of the data for out-of-bag validation of the final model while the cross-validation happens within the hyperparemeter tunning\n",
        "index = list(range(len(np_spect)))\n",
        "index10 = rdm.sample(index,math.ceil(len(index)*.1))\n",
        "index90 = [x for x in index if x not in index10]\n",
        "\n",
        "#selecting the groups for training (tr) and validation (vl)\n",
        "tr_trait_df = np_trait[index90,]\n",
        "tr_spect_df = np_spect[index90,]\n",
        "\n",
        "vl_trait_df = np_trait[index10,]\n",
        "vl_spect_df = np_spect[index10,]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A127epk7P__n"
      },
      "source": [
        "Definining the optimization function and the search space\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gr2pmF5-P9dr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ddf10023-8ca9-49fd-830e-9b4690ea6f85"
      },
      "source": [
        "\n",
        "#objective function\n",
        "\n",
        "#first we define the function\n",
        "N_FOLDS = 10\n",
        "MAX_EVALS = 200\n",
        "\n",
        "#here we can just fetch the data so we keep everything in one place\n",
        "train_labels = tr_trait_df\n",
        "train_features = tr_spect_df \n",
        "\n",
        "from sklearn.model_selection import KFold # import KFold\n",
        "\n",
        "def objective(params, n_folds = N_FOLDS):\n",
        "    \"\"\"Objective function for Random forest Hyperparameter Tuning\"\"\"\n",
        "\n",
        "    # Perform n_fold cross validation with hyperparameters\n",
        "    gp = GaussianProcessRegressor(**params, random_state = 42) #normalizing and adding noise does not increase the likelihood of sucessfull minimization\n",
        "\n",
        "    #scores = cross_val_score(gp, X=train_features, y=train_labels, cv=10, scoring='neg_mean_absolute_error') #thi is the simple Mean abs\n",
        "    #the cross_val_score function in GP seems to work out wonky so lets try explicitely creating it\n",
        "\n",
        "    kf = KFold(n_splits=N_FOLDS,shuffle=True,random_state=42)\n",
        "\n",
        "    scores = np.empty((0,n_traits)) #an empty arrray to holdout all the results of the kfoldind\n",
        "    for train_index,test_index in kf.split(train_labels):\n",
        "      x_train_k,x_test_k = train_features[train_index],train_features[test_index]\n",
        "      y_train_k,y_test_k = train_labels[train_index],train_labels[test_index]\n",
        "\n",
        "      gp.fit(x_train_k,y_train_k)\n",
        "      outval = gp.predict(x_test_k)\n",
        "\n",
        "      #storing all the MAE scores into a single place - this can be changed to anoter function\n",
        "      score = [metrics.mean_absolute_error(y_test_k[:,0], outval[:,0]),\n",
        "               metrics.mean_absolute_error(y_test_k[:,1], outval[:,1]),\n",
        "               metrics.mean_absolute_error(y_test_k[:,2], outval[:,2]),\n",
        "               metrics.mean_absolute_error(y_test_k[:,3], outval[:,3])]\n",
        "      scores=np.append(scores,score) #just add the values to the end of the numpy and continues\n",
        "\n",
        "\n",
        "    # Extract the best score\n",
        "    best_score = max(abs(scores)) #using the abs here forces the function to minimize independently of the sign\n",
        "    # if the the result is miniming\n",
        "\n",
        "    # Loss must be minimized\n",
        "    #loss = 1 - best_score \n",
        "    loss = best_score \n",
        "\n",
        "    # Dictionary with information for evaluation\n",
        "    return {'loss': loss, 'params': params, 'status': STATUS_OK}\n",
        "\n",
        "#distribution functions for the parameter sampling are here:\n",
        "#https://github.com/hyperopt/hyperopt/wiki/FMin\n",
        "\n",
        "\n",
        "#first we should create the kernels\n",
        "#K_white = kernels.WhiteKernel()\n",
        "\n",
        "#seems trivial to limit the maximum number of iterations within this base function\n",
        "#https://scikit-optimize.github.io/stable/_modules/sklearn/gaussian_process/_gpr.html\n",
        "#or alternitively call an altered version of the scipy.optimizer (or with an improved optimization technique)\n",
        "\n",
        "\n",
        "#RBF,DotProduct,Matern,ExpSineSquared,RationalQuadratic\n",
        "#K_rbf = 1.0*kernels.RBF()\n",
        "K_rbf = RBF(length_scale_bounds=\"fixed\")\n",
        "#K_ratQ = kernels.RationalQuadratic() \n",
        "K_ratQ = RationalQuadratic(length_scale_bounds=\"fixed\",alpha_bounds=\"fixed\")\n",
        "#k_mate = 1.0*kernels.Matern()\n",
        "k_mate = Matern(length_scale_bounds=\"fixed\")\n",
        "#K_expsine = kernels.ExpSineSquared()\n",
        "#K_expsine = ExpSineSquared(length_scale_bounds=\"fixed\",periodicity_bounds=\"fixed\")\n",
        "K_dotprod = DotProduct()\n",
        "K_dotprod = DotProduct(sigma_0_bounds=\"fixed\")\n",
        "\n",
        "#kernel_list = [K_ratQ,K_rbf,k_mate,K_expsine,K_dotprod]\n",
        "#kernel_list = [RBF(1.0, length_scale_bounds=\"fixed\")] #ahh! eureka, this is the version of RBF it utilized by default -> it only has to minimize one thing, basically its very slow because of that..\n",
        "kernel_list = [K_rbf,K_ratQ,k_mate,K_dotprod]\n",
        "\n",
        "space = {\n",
        "    'n_restarts_optimizer': hp.choice('n_restarts_optimizer', range(10, 101, 10)), #to avoid values of 0\n",
        "    'kernel': hp.choice('kernel', kernel_list),\n",
        "    'normalize_y':hp.choice('normalize_y',[True,False])\n",
        "}\n",
        "\n",
        "for i in range(10, 101, 10):\n",
        "  print(i, end=', ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10, 20, 30, 40, 50, 60, 70, 80, 90, 100, "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEf-kvGcVtNi"
      },
      "source": [
        "This now is the order for running"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iGm-b55UXNE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6f3952ed-5c16-4c81-96f1-5c6c884d852c"
      },
      "source": [
        "#this loop is the actual testing\n",
        "bayes_trials = Trials()\n",
        "\n",
        "best_pgr = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = MAX_EVALS, trials = bayes_trials,verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [05:39<00:00,  1.70s/it, best loss: 0.028436356979695123]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5qy0xyz8qLf"
      },
      "source": [
        "Checking the outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MBrznXC8sKH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2c5b9e7a-772c-4ad6-9280-5789b22cae26"
      },
      "source": [
        "best_pgr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kernel': 1, 'n_restarts_optimizer': 7, 'normalize_y': 0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    }
  ]
}