{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RTM Noise inversion Optimized.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoqFMzPxvKtv"
      },
      "source": [
        "*this analysis is in almost everything similar to the pure pixel except for extra steps for noise generation*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXrTRvFmvT7P"
      },
      "source": [
        "# Step 1: mount the google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEjeVbOdlsil",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "03abcc1b-c5fd-400e-86e1-84333982e3dd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#Perhaps this step can be skipped by saving directly to the workspace"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjJ3dsjEvX3Y"
      },
      "source": [
        "# Step 2: install the packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdXzp3BNvd69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "748e79fe-1bee-49a8-c7e4-c205d2d1bdb8"
      },
      "source": [
        "#package instalation\n",
        "\n",
        "#Installing PROSAIL\n",
        "!pip install prosail\n",
        "\n",
        "#latin hypercube stuff\n",
        "#lets try to do a LHS\n",
        "!pip install lhsmdu\n",
        "\n",
        "#this package as a number of functions to deal with hyperspectral data\n",
        "!pip install pysptools"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting prosail\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b5/52/d0c15ab469e8c82bc76a6b6cd614efbc60e43d09d5bacaa349170d229e91/prosail-2.0.5-py3-none-any.whl (149kB)\n",
            "\r\u001b[K     |██▏                             | 10kB 15.0MB/s eta 0:00:01\r\u001b[K     |████▍                           | 20kB 1.6MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 30kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 40kB 2.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 81kB 2.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 92kB 2.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 102kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 112kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 122kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 133kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 143kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 153kB 2.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from prosail) (1.4.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from prosail) (0.48.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from prosail) (3.6.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from prosail) (1.18.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->prosail) (46.4.0)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->prosail) (0.31.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (1.12.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (8.3.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (1.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (1.8.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (19.3.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (0.7.1)\n",
            "Installing collected packages: prosail\n",
            "Successfully installed prosail-2.0.5\n",
            "Collecting lhsmdu\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/f0/e714a4dae734bcd7228a09d74fff7dc5857dc3311cd72a3e07b09c85d088/lhsmdu-0.1-py3-none-any.whl\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lhsmdu) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lhsmdu) (1.18.4)\n",
            "Installing collected packages: lhsmdu\n",
            "Successfully installed lhsmdu-0.1\n",
            "Collecting pysptools\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/20/cef48129eff2bdcb282279138c09e6f04770a8fdcb3c1bb9a98fe4086d2d/pysptools-0.15.0.tar.gz (8.1MB)\n",
            "\u001b[K     |████████████████████████████████| 8.1MB 2.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pysptools\n",
            "  Building wheel for pysptools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pysptools: filename=pysptools-0.15.0-cp36-none-any.whl size=8133750 sha256=30a1ae6fd3e25414fb357edfaeff69e51aa982caf6808263327bdfe01164d6c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/60/be/a6719d91bfa59135201feb034c7069e4146aa576fc0dc9e624\n",
            "Successfully built pysptools\n",
            "Installing collected packages: pysptools\n",
            "Successfully installed pysptools-0.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qllJ_ubGvf7V"
      },
      "source": [
        "#TensorFlow update correction\n",
        "\n",
        "Tensor flow has been updated so we will uninstall the new version and push in the old version which works well on my case\n",
        "\n",
        "- Notice: this only has to be run if tf > 1.4.0\n",
        "- if you do this after you load the package, everything will have to be restarted"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GU03VLJvs5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "f3fb540f-a834-40fd-9c89-c846b5a9ad13"
      },
      "source": [
        "#!pip uninstall tensorflow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.2.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow-2.2.0.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RC2YcquSv3JX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "outputId": "63766ecc-6fed-4b9f-a8ce-7a4bc7cc9880"
      },
      "source": [
        "#!pip install tensorflow==1.14.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n",
            "\u001b[K     |████████████████████████████████| 109.2MB 105kB/s \n",
            "\u001b[?25hCollecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 48.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (3.10.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.34.2)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.3.3)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.8.1)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 53.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.12.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.29.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.12.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.18.4)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.9.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (46.3.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.1.0)\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.2.1\n",
            "    Uninstalling tensorboard-2.2.1:\n",
            "      Successfully uninstalled tensorboard-2.2.1\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDaOFuhRwWL9"
      },
      "source": [
        "# Step 3 Import the packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPWmSuBawUCH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "cdc3e338-1cf4-49a7-fd3f-31a57508df16"
      },
      "source": [
        "#General purpose: \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from scipy import stats\n",
        "\n",
        "#the beutiful R like data frame\n",
        "import pandas as pd\n",
        "\n",
        "#PROSPECT+SAIL Radiative transfer mode package\n",
        "import prosail\n",
        "\n",
        "#Sampling design package\n",
        "import lhsmdu\n",
        "\n",
        "#a few more stuff for random\n",
        "import random as rdm\n",
        "import math\n",
        "\n",
        "\n",
        "\n",
        "#package to for operations on spectral data\n",
        "import pysptools as sptool \n",
        "from pysptools import distance\n",
        "\n",
        "\n",
        "from sklearn.model_selection import KFold # import KFold\n",
        "\n",
        "#machine learning stuff\n",
        "#NEURAL NETWORK - Keras will be updated soon so this colab will also have to be changed\n",
        "\n",
        "from sklearn.neural_network import MLPRegressor as ANN_reg #this is a simpler neural network package\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Input\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "#ignore the warning for now\n",
        "\n",
        "#Random FOREST\n",
        "# Fitting Random Forest Regression to the dataset \n",
        "# import the regressor \n",
        "from sklearn.ensemble import RandomForestRegressor \n",
        "\n",
        "#Gaussian processes\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import ConstantKernel, Matern, WhiteKernel, ExpSineSquared, RationalQuadratic, RBF\n",
        "\n",
        "#aux functions\n",
        "from sklearn.preprocessing import MinMaxScaler #this is to standardize the input data [not used for now]\n",
        "from sklearn import metrics\n",
        "\n",
        "#for model storing -sklearn\n",
        "import pickle\n",
        "\n",
        "#for model storing keras\n",
        "from keras.models import model_from_json\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from tensorflow import metrics\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation\n",
        "from keras.optimizers import Adadelta, Adam, rmsprop\n",
        "\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Input\n",
        "from keras.models import Sequential, Model\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from sklearn.model_selection import KFold \n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from numpy import savetxt,loadtxt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QDHFKu-Ksp-"
      },
      "source": [
        "# Step 4.1: set up of custom auxiliar functions\n",
        "\n",
        "- First we have to set of custom function to facilitate the loop calls\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agdA-BkqKZLQ"
      },
      "source": [
        "def custom_prosail(cab,cw,cm,lai):\n",
        "  import prosail\n",
        "  #default parameters\n",
        "  n= 1.\n",
        "  car=10.\n",
        "  cbrown=0.01\n",
        "  typelidf=1 #this is the default option\n",
        "  lidfa = -1 #leaf angle distribution parameter a and b\n",
        "  lidfb=-0.15\n",
        "  hspot= 0.01 #hotspot parameters - got this from R package https://www.rdocumentation.org/packages/hsdar/versions/0.4.1/topics/PROSAIL\n",
        "  #sun and viewing angle\n",
        "  tts=30. #observation and solar position parameters\n",
        "  tto=10. \n",
        "  psi=0.\n",
        "  #for now i put them by hand but they should be an input of a custom function\n",
        "  #tts=sol_zen #solar zenith angle\n",
        "  #tto=inc_zen #sensor zenith angle\n",
        "  #psi=raa\n",
        "  rho_out = prosail.run_prosail(n,\n",
        "                                 cab,\n",
        "                                 car,\n",
        "                                 cbrown,\n",
        "                                 cw,\n",
        "                                 cm,\n",
        "                                 lai,\n",
        "                                 lidfa,\n",
        "                                 hspot,\n",
        "                                 tts,tto,psi,\n",
        "                                 typelidf, lidfb,\n",
        "                                 prospect_version=\"D\",\n",
        "                                 factor='SDR', \n",
        "                                 rsoil=.5, psoil=.5)\n",
        "  return(rho_out)\n",
        "\n",
        "def Prosail2S2(path2csv,spectra_input):\n",
        "  #importing pandas\n",
        "  import pandas as pd\n",
        "  import numpy\n",
        "  import numpy as np\n",
        "  #upload a S2_Response.csv from https://earth.esa.int/web/sentinel/user-guides/sentinel-2-msi/document-library/-/asset_publisher/Wk0TKajiISaR/content/sentinel-2a-spectral-responses\n",
        "\n",
        "  s2_table = pd.read_csv(path2csv,sep=\";\",decimal=\",\") #check if this is proper, regarding the sep and dec\n",
        "  #chekc which row you are actually extracting\n",
        "\n",
        "  s2_table_sel = s2_table[s2_table['SR_WL'].between(400,2500)] #selects all values between 400 and 2500\n",
        "  spectra_input_df = pd.DataFrame(data=spectra_input,columns=[\"rho\"],index=s2_table_sel.index) #transforms the input array into a pandas df with the column name rho and row.index = to the original input table\n",
        "\n",
        "  \n",
        "  rho_s2 = s2_table_sel.multiply(spectra_input_df['rho'],axis=\"index\") #calculates the numerator\n",
        "  w_band_sum = s2_table_sel.sum(axis=0,skipna = True) #calculates the denominator\n",
        "\n",
        "  output = (rho_s2.sum(axis=0)/w_band_sum).rename_axis(\"ID\").values #runs the weighted mean and converts the output to a numpy array\n",
        "\n",
        "  return output[1:] #removes the first value because it represents the wavelength column\n",
        "\n",
        "#please LOAD THTE FILE NOW\n",
        "filepath=\"/content/drive/My Drive/S2_Response.csv\"\n",
        "#filepath=\"/content/S2_Responses_S2B.csv\"\n",
        "#filepath=\"/content/drive/My Drive/S2_Response.csv\"\n",
        "\n",
        "\n",
        "def Gen_spectra_data(traits):\n",
        "  k = 1\n",
        "  #pd_train_traits=traits\n",
        "  #print(range(len(traits)))\n",
        "  for i in range(len(traits)):\n",
        "    #n_t = pd_train_traits[\"n\"][i]\n",
        "    cab_t = traits[\"cab\"][i]\n",
        "    #car_t = pd_train_traits[\"car\"][i]\n",
        "    #cbrown_t = pd_train_traits[\"cbrown\"][i]\n",
        "    cw_t = traits[\"cw\"][i]\n",
        "    cm_t = traits[\"cm\"][i]\n",
        "    lai_t = traits[\"lai\"][i]\n",
        "\n",
        "    if k == 1:\n",
        "      tr_rho_s = custom_prosail(cab_t,cw_t,cm_t,lai_t)\n",
        "      tr_rho_s = Prosail2S2(filepath,tr_rho_s)\n",
        "      #plt.plot ( x, tr_rho_s, ':', label=\"Training prosail\")\n",
        "      #plt.legend(loc='best')\n",
        "      \n",
        "    if k > 1:\n",
        "      tr_rho_t = custom_prosail(cab_t,cw_t,cm_t,lai_t)\n",
        "      tr_rho_t = Prosail2S2(filepath,tr_rho_t)\n",
        "      tr_rho_s = np.vstack((tr_rho_s,tr_rho_t))\n",
        "      #plt.plot ( x, tr_rho_t, ':')\n",
        "\n",
        "    k = k+1\n",
        "\n",
        "\n",
        "  rho_samples=tr_rho_s\n",
        "\n",
        "\n",
        "  return rho_samples\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Function to store the outputs into a table\n",
        "column_names=[\"Model\",\n",
        "              \"NoiseLevel\",\n",
        "              \"NoiseType\",\n",
        "              \"OutOfBag\",\n",
        "              \"KFold_tr_samples\",\n",
        "              \"KFold_vl_samples\",\n",
        "              \"Variable\",\n",
        "              \"Fold_nr\",\n",
        "              \"ExplVar\",\n",
        "              \"Max_err\",\n",
        "              \"Mean_abs_Err\",\n",
        "              \"Mean_sqr_err\",\n",
        "              #\"Mean_sqr_lg_err\",\n",
        "              \"Median_abs_err\",\n",
        "              \"r2\",\n",
        "              \"MAPE\",\n",
        "              \"ModelName\"]\n",
        "              #\"Mean_poiss_dev\",\n",
        "              #\"Mean_gamma_dev\"]\n",
        "              #\"Mean_tweed_dev\"]\n",
        "\n",
        "#mape is not existant in the package so we have to create it:\n",
        "#https://stats.stackexchange.com/questions/58391/mean-absolute-percentage-error-mape-in-scikit-learn\n",
        "#from sklearn.utils import check_array\n",
        "def mean_absolute_percentage_error(y_true, y_pred): \n",
        "\n",
        "    ## Note: does not handle mix 1d representation\n",
        "    #if _is_1d(y_true): \n",
        "    #    y_true, y_pred = _check_1d_array(y_true, y_pred)\n",
        "\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "  #Here, the file is used for saving\n",
        "  #creating a df to receive the data\n",
        "\n",
        "\n",
        "def calc_metrics(MDL,NoiseLevel,NoiseType,oob_samples,kf_tr,kf_vl,Variable,Fold,Ref,Pred,Modelname):\n",
        "\n",
        "  out_list = {\"Model\":MDL,\n",
        "              \"NoiseLevel\":NoiseLevel,\n",
        "              \"NoiseType\":NoiseType,\n",
        "              \"OutOfBag\":oob_samples,\n",
        "              \"KFold_tr_samples\":kf_tr,\n",
        "              \"KFold_vl_samples\":kf_vl,\n",
        "              \"Variable\":Variable,\n",
        "              \"Fold_nr\":Fold,\n",
        "              \"ExplVar\": metrics.explained_variance_score(Ref, Pred),\n",
        "              \"Max_err\": metrics.max_error(Ref, Pred),\n",
        "              \"Mean_abs_Err\": metrics.mean_absolute_error(Ref, Pred),\n",
        "              \"Mean_sqr_err\": metrics.mean_squared_error(Ref, Pred),\n",
        "              \"Median_abs_err\" : metrics.median_absolute_error(Ref, Pred),\n",
        "              \"r2\": metrics.r2_score(Ref, Pred),\n",
        "              \"MAPE\": mean_absolute_percentage_error(Ref, Pred),\n",
        "              \"ModelName\":Modelname}\n",
        "\n",
        "\n",
        "  return out_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9i1rX0DMA8G"
      },
      "source": [
        "# Extra function for noise generation\n",
        "\n",
        "Combined noise & inverted noise based on the equations 4 and 5 on: \n",
        "\n",
        "https://www.mdpi.com/2072-4292/7/8/10321/html#B31-remotesensing-07-10321\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pg-GpwaMYQm"
      },
      "source": [
        "def combined_noise(ref,sigma):\n",
        "\n",
        "  #ref is an input vector and sigma is the standar deviation of the noise\n",
        "\n",
        "  #this format implies some values go negative or above 1 which is terrible\n",
        "\n",
        "  #the multiplier parameter is solid\n",
        "  #mp_noise = ref*(1 + np.random.normal(0,scale=2*sigma,size=ref.shape[0]))\n",
        "\n",
        "  #this form makes the value go over 1 and under 0 from time to time\n",
        "  #ad_noise = 0#+np.random.normal(0,sigma)\n",
        "\n",
        "  #truncated version - the limits are on 2*sigma to give it a leeway \n",
        "  #ad_noise = 0+stats.truncnorm.rvs(0-sigma*2,0+sigma*2,loc=0,scale=sigma,size=ref.shape[0])\n",
        "\n",
        "  #the only way i can think to force it doesn't go over 1 or under 0, is to use a naive verifier\n",
        "  \n",
        "  #sigma is any value between 0 and 1, reflecting \"% percent error\"\n",
        "  #notice i give leeway to sigma by doubling the interval but the truncated version should be stronger againt bad values\n",
        "  \n",
        "  mp_noise = ref*(1 + stats.truncnorm.rvs(0-sigma*4,0+sigma*4,loc=0,scale=sigma*2,size=ref.shape[0]))\n",
        "  ad_noise = 0+stats.truncnorm.rvs(0-sigma*2,0+sigma*2,loc=0,scale=sigma,size=ref.shape[0])\n",
        "  out_ref = mp_noise+ ad_noise\n",
        "\n",
        "\n",
        "  #this looks fun but breaks the speed of processing....\n",
        "  # while max(out_ref) > 1 or min(out_ref)<0:\n",
        "  #   #print(\"i am in the loop!\")\n",
        "  #   mp_noise = ref*(1 + stats.truncnorm.rvs(0-sigma*4,0+sigma*4,loc=0,scale=sigma*2,size=ref.shape[0]))\n",
        "  #   ad_noise = 0+stats.truncnorm.rvs(0-sigma*2,0+sigma*2,loc=0,scale=sigma,size=ref.shape[0])\n",
        "  #   out_ref = mp_noise+ ad_noise\n",
        "  #   #print(out_ref)\n",
        "\n",
        "  #making everything that goes under, be 0 or 1\n",
        "  out_ref[out_ref>1]=1\n",
        "  out_ref[out_ref<0]=0\n",
        "\n",
        "  return out_ref\n",
        "\n",
        "def inverted_noise(ref,sigma):\n",
        "\n",
        "  #ref is an input vector and sigma is the standar deviation of the noise\n",
        "\n",
        "\n",
        "  #in this case, both of these are producing improper results - less than 0 or more than 1 oftne. \n",
        "  #mp_noise = 1-(abs(1-ref)*(1 + np.random.normal(0,scale=2*sigma,size=ref.shape[0])))\n",
        "  #ad_noise = 0#+np.random.normal(0,sigma)\n",
        "\n",
        "  mp_noise = 1-(1-ref)*(1 + stats.truncnorm.rvs(0-sigma*4,0+sigma*4,loc=0,scale=sigma*2,size=ref.shape[0]))\n",
        "  ad_noise = 0+stats.truncnorm.rvs(0-sigma*2,0+sigma*2,loc=0,scale=sigma,size=ref.shape[0])\n",
        "\n",
        "  out_ref = mp_noise+ ad_noise\n",
        "\n",
        "  #this looks fun but breaks the speed of processing...\n",
        "  # while max(out_ref) > 1 or min(out_ref)<0:\n",
        "  #   #print(\"i am in the loop!\")\n",
        "  #   mp_noise = 1-(1-ref)*(1 + stats.truncnorm.rvs(0-sigma*4,0+sigma*4,loc=0,scale=sigma*2,size=ref.shape[0]))\n",
        "  #   ad_noise = 0+stats.truncnorm.rvs(0-sigma*2,0+sigma*2,loc=0,scale=sigma,size=ref.shape[0])\n",
        "  #   out_ref = mp_noise+ ad_noise\n",
        "  #   #print(out_ref)  while \n",
        "\n",
        "\n",
        "  #making everything that goes under, be 0 or 1\n",
        "  out_ref[out_ref>1]=1\n",
        "  out_ref[out_ref<0]=0\n",
        "\n",
        "  return out_ref\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WFsIMjrNjD3"
      },
      "source": [
        "For testing: generating noise for one run of prosail"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrxpfpcfCrIy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "b03dc419-684f-468c-c00a-0f237c854a3a"
      },
      "source": [
        "cab = 80\n",
        "cw  = 0.006\n",
        "cm  = 0.006\n",
        "lai = 3 \n",
        "#creating input\n",
        "trait_dic = {\"cab\":[cab],\"cw\":[cw],\"cm\":[cm],\"lai\":[lai]}\n",
        "trait_pd = pd.DataFrame.transpose(pd.DataFrame([cab,cw,cm,lai]))\n",
        "trait_pd.columns = [\"cab\",\"cw\",\"cm\",\"lai\"]\n",
        "trait_pd\n",
        "np_spect = Gen_spectra_data(trait_pd)[[1,2,3,4,5,6,8,11,12]]\n",
        "np_spect.shape\n",
        "#plot the \"pure signal\"\n",
        "np_spect_noise1 = combined_noise(np_spect,.1) #0.05 of error\n",
        "np_spect_noise2 = inverted_noise(np_spect,.1) #0.05 of error\n",
        "x_axis = range(9)\n",
        "fig = plt.figure()\n",
        "ax1 = fig.add_subplot(111)\n",
        "#,label=\"Pure\"\n",
        "ax1.scatter(x_axis,np_spect,label=\"Pure\")\n",
        "ax1.scatter(x_axis,np_spect_noise1,label=\"Combined\")\n",
        "ax1.scatter(x_axis,np_spect_noise2,label=\"Inverted\")\n",
        "plt.legend(loc =\"upper left\")\n",
        "plt.show()\n",
        "print(np_spect)\n",
        "print(np_spect_noise1)\n",
        "print(np_spect_noise2)\n",
        "#rule of thumb the error on the inverted should be higher in the first bands "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[0.02246956 0.02850029 0.01793622 0.04028608 0.31156409 0.47324267\n",
            " 0.47773286 0.28741394 0.11398075]\n",
            "[0.00743674 0.02180222 0.02173483 0.02516667 0.30189327 0.43433183\n",
            " 0.52141294 0.3106867  0.13361433]\n",
            "[0.         0.00625446 0.05281358 0.00380104 0.29189236 0.50384317\n",
            " 0.52213677 0.25558124 0.17986032]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xByKT_DEJw9t"
      },
      "source": [
        "# Step 4.2: Setting up machine learning models \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_D3QZGgkHwJ"
      },
      "source": [
        "\n",
        "#initializing the random forest\n",
        "#optimized\n",
        "\n",
        "from sklearn.gaussian_process.kernels import RBF,DotProduct,Matern,ExpSineSquared,RationalQuadratic\n",
        "\n",
        "#Random forest\n",
        "#initializing the random forest\n",
        "#optimized\n",
        "\n",
        "def rfr_call():\n",
        "\n",
        "  mdl = RandomForestRegressor(n_estimators=850,random_state=0,\n",
        "                               min_samples_leaf=0.0005292768087778988,\n",
        "                               min_samples_split=0.0028599513810681296,\n",
        "                               verbose=0) \n",
        "  return mdl\n",
        "\n",
        "#test cals to the function\n",
        "rfr_ml_comb = rfr_call()\n",
        "rfr_ml_inve = rfr_call()\n",
        "\n",
        "#gaussian processes with default acquisition function\n",
        "#initializing the gaussian process -optimized\n",
        "K_ratQ = RationalQuadratic(length_scale_bounds=\"fixed\",alpha_bounds=\"fixed\")\n",
        "\n",
        "def gpr_call(noise_val):\n",
        "  mdl = GaussianProcessRegressor(kernel=K_ratQ,\n",
        "                                 alpha=noise_val,\n",
        "                                 n_restarts_optimizer=80, # -> 1*RBF(1.0) is the default kerneºl\n",
        "                                 normalize_y=True,\n",
        "                                 random_state=0,\n",
        "                                 )\n",
        "  \n",
        "  return mdl\n",
        "\n",
        "#test calls to the function \n",
        "gpr_ml_comb = gpr_call(.5)\n",
        "gpr_ml_inve = gpr_call(.5) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2yUMhFDzKuG"
      },
      "source": [
        "Setting up the neural networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5-aWzNczMbf"
      },
      "source": [
        "hiddenlayer = {0:'one',1:'two',2:'three'}\n",
        "modeltype = {0: 'MTST',1: 'MTSTST',2: 'MTMTST',3: 'MTMTSTST'}\n",
        "activation_f = {0:'linear', 1:'sigmoid', 2:'tanh',3:'exponential',4:'softplus',5:'relu',6:'softsign'}\n",
        "units_shared = {0: 5,1: 10,2: 15,3: 20}\n",
        "units_traits = {0:1,1:2,2:3,3:4,4:5,5:6,6:7,7:8,8:9,9:10}\n",
        "optimizer = {0: 'adam',1: 'RMSprop',2: 15,3: 'Adadelta'}\n",
        "\n",
        "param_dict_st_ann = {'aa_hiddenlayers':hiddenlayer[2],\n",
        "              'activation_3_1': activation_f[6],\n",
        "              'activation_3_2': activation_f[6],\n",
        "              'activation_3_3': activation_f[5],\n",
        "              'optimizer_3': optimizer[0],\n",
        "              'units_3_1': units_shared[3],\n",
        "              'units_3_2': units_shared[3],\n",
        "              'units_3_3': units_shared[2]}\n",
        "#param_dict_st_ann\n",
        "\n",
        "param_dict_mtn = {'aaa_modeltype': modeltype[3],\n",
        "              'activation_MTMTSTST': activation_f[2],\n",
        "              'activation_MTMTSTST2': activation_f[6],\n",
        "              'activation_cab_MTMTSTST1': activation_f[4],'activation_cab_MTMTSTST2': activation_f[1],\n",
        "              'activation_cm_MTMTSTST1': activation_f[5],'activation_cm_MTMTSTST2': activation_f[1],\n",
        "              'activation_cw_MTMTSTST1': activation_f[4],'activation_cw_MTMTSTST2': activation_f[2],\n",
        "              'activation_lai_MTMTSTST1': activation_f[6],'activation_lai_MTMTSTST2': activation_f[6],\n",
        "              'units_cab_MTMTSTST1': units_traits[2],'units_cab_MTMTSTST2': units_traits[5],\n",
        "              'units_cm_MTMTSTST1': units_traits[7],'units_cm_MTMTSTST2': units_traits[5],\n",
        "              'units_cw_MTMTSTST1': units_traits[2],'units_cw_MTMTSTST2': units_traits[4],\n",
        "              'units_lai_MTMTSTST1': units_traits[5],'units_lai_MTMTSTST2': units_traits[9],\n",
        "              'units_shared_MTMTSTST': units_shared[2],\n",
        "              'units_shared_MTMTSTST2': units_shared[3],\n",
        "              'optimizer_MTMTSTST':'adam'}\n",
        "#param_dict_mtn\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRnGNuC0zSDM"
      },
      "source": [
        "#optimized layer -> notice that 3 hidden layers were selected, so the combination of place 0, 2, and 3 is to be used\n",
        "def st_ann(params):\n",
        "\n",
        "  from keras.models import Sequential\n",
        "  from keras.layers.core import Dense, Activation\n",
        "  from keras.optimizers import Adadelta, Adam, rmsprop\n",
        "\n",
        "  mdl = Sequential()\n",
        "  #mdl.add(Dense(units=params['units_1'],input_dim=9,activation=params['activation_1']))\n",
        "  #mdl.add(Activation(params['activation_1']))\n",
        "  #print(\"This is the dictionary\")\n",
        "  #print(params)\n",
        "\n",
        "  hiddenlayers = units=params['aa_hiddenlayers']\n",
        "  \n",
        "  if hiddenlayers == 'one':\n",
        "    mdl.add(Dense(units=params['units_1_1'],input_dim=9,activation=params['activation_1_1']))\n",
        "\n",
        "    #and the output layer\n",
        "    mdl.add(Dense(4,activation='linear'))\n",
        "    #mdl.add(Activation(\"linear\"))\n",
        "    mdl.compile(loss='mean_squared_error', optimizer=params['optimizer_1'], metrics=['mean_squared_error'])\n",
        "  \n",
        "  if hiddenlayers == 'two':\n",
        "    mdl.add(Dense(units=params['units_2_1'],input_dim=9,activation=params['activation_2_1']))\n",
        "    mdl.add(Dense(units=params['units_2_2'],activation=params['activation_2_2']))\n",
        "\n",
        "    #and the output layer\n",
        "    mdl.add(Dense(4,activation='linear'))\n",
        "    #mdl.add(Activation(\"linear\"))\n",
        "    mdl.compile(loss='mean_squared_error', optimizer=params['optimizer_2'], metrics=['mean_squared_error'])\n",
        "    \n",
        "  if hiddenlayers == 'three':\n",
        "    mdl.add(Dense(units=params['units_3_1'],input_dim=9,activation=params['activation_3_1']))\n",
        "    mdl.add(Dense(units=params['units_3_2'],activation=params['activation_3_2']))\n",
        "    mdl.add(Dense(units=params['units_3_3'],activation=params['activation_3_3']))\n",
        "\n",
        "    #and the output layer\n",
        "    mdl.add(Dense(4,activation='linear'))\n",
        "    #mdl.add(Activation(\"linear\"))\n",
        "    mdl.compile(loss='mean_squared_error', optimizer=params['optimizer_3'], metrics=['mean_squared_error'])\n",
        "\n",
        "\n",
        "  return mdl\n",
        "\n",
        "def mt_ann(params):\n",
        "\n",
        "  #mdl = Sequential()\n",
        "  #mdl.add(Dense(units=params['units_1'],input_dim=9,activation=params['activation_1']))\n",
        "  #mdl.add(Activation(params['activation_1']))\n",
        "  #print(\"This is the dictionary\")\n",
        "  #print(params)\n",
        "\n",
        "  inputs = Input(shape=(9,))\n",
        "\n",
        "  #model selector\n",
        "  struct = params['aaa_modeltype']\n",
        "  if struct == 'MTST':\n",
        "    shared_MTST = Dense(units=params['units_shared_MTST'],activation=params['activation_MTST'])(inputs)\n",
        "    cab_MTST = Dense(units=params['units_cab_MTST'], activation=params['activation_cab_MTST'])(shared_MTST)\n",
        "    cw_MTST  = Dense(units=params['units_cw_MTST'],  activation=params['activation_cw_MTST'])(shared_MTST)\n",
        "    cm_MTST  = Dense(units=params['units_cm_MTST'],  activation=params['activation_cm_MTST'])(shared_MTST)\n",
        "    lai_MTST = Dense(units=params['units_lai_MTST'], activation=params['activation_lai_MTST'])(shared_MTST)\n",
        "\n",
        "    #this is a just a final output layer needed for the whole thing to work\n",
        "    cab_out = Dense(1, activation='linear')(cab_MTST)\n",
        "    cw_out  = Dense(1, activation='linear')(cw_MTST)\n",
        "    cm_out  = Dense(1, activation='linear')(cm_MTST)\n",
        "    lai_out = Dense(1, activation='linear')(lai_MTST)\n",
        "\n",
        "    mdl = Model(inputs=inputs, outputs=[cab_out,cw_out,cm_out,lai_out])\n",
        "    mdl.compile(loss='mean_squared_error', optimizer=params['optimizer_MTST'], metrics=['mean_squared_error'])\n",
        "\n",
        "\n",
        "  if struct == 'MTSTST':\n",
        "    shared_MTSTST = Dense(units=params['units_shared_MTSTST'],activation=params['activation_MTSTST'])(inputs)\n",
        "    cab_MTSTST = Dense(units=params['units_cab_MTSTST1'], activation=params['activation_cab_MTSTST1'])(shared_MTSTST)\n",
        "    cw_MTSTST  = Dense(units=params['units_cw_MTSTST1'],  activation=params['activation_cw_MTSTST1'])(shared_MTSTST)\n",
        "    cm_MTSTST  = Dense(units=params['units_cm_MTSTST1'],  activation=params['activation_cm_MTSTST1'])(shared_MTSTST)\n",
        "    lai_MTSTST = Dense(units=params['units_lai_MTSTST1'], activation=params['activation_lai_MTSTST1'])(shared_MTSTST)\n",
        "\n",
        "    cab_MTSTST2 = Dense(units=params['units_cab_MTSTST2'], activation=params['activation_cab_MTSTST2'])(cab_MTSTST)\n",
        "    cw_MTSTST2  = Dense(units=params['units_cw_MTSTST2'],  activation=params['activation_cw_MTSTST2'])(cw_MTSTST)\n",
        "    cm_MTSTST2  = Dense(units=params['units_cm_MTSTST2'],  activation=params['activation_cm_MTSTST2'])(cm_MTSTST)\n",
        "    lai_MTSTST2 = Dense(units=params['units_lai_MTSTST2'], activation=params['activation_lai_MTSTST2'])(lai_MTSTST)\n",
        "\n",
        "    #this is a just a final output layer needed for the whole thing to work\n",
        "    cab_out = Dense(1, activation='linear')(cab_MTSTST2)\n",
        "    cw_out  = Dense(1, activation='linear')(cw_MTSTST2)\n",
        "    cm_out  = Dense(1, activation='linear')(cm_MTSTST2)\n",
        "    lai_out = Dense(1, activation='linear')(lai_MTSTST2)\n",
        "\n",
        "    mdl = Model(inputs=inputs, outputs=[cab_out,cw_out,cm_out,lai_out])\n",
        "    mdl.compile(loss='mean_squared_error', optimizer=params['optimizer_MTSTST'], metrics=['mean_squared_error'])\n",
        "\n",
        "  if struct == 'MTMTST':\n",
        "    shared_MTMTST = Dense(units=params['units_shared_MTMTST'],activation=params['activation_MTMTST'])(inputs)\n",
        "    shared_MTMTST2 = Dense(units=params['units_shared_MTMTST2'],activation=params['activation_MTMTST2'])(shared_MTMTST)\n",
        "\n",
        "    cab_MTMTST = Dense(units=params['units_cab_MTMTST'], activation=params['activation_cab_MTMTST'])(shared_MTMTST2)\n",
        "    cw_MTMTST  = Dense(units=params['units_cw_MTMTST'],  activation=params['activation_cw_MTMTST'])(shared_MTMTST2)\n",
        "    cm_MTMTST  = Dense(units=params['units_cm_MTMTST'],  activation=params['activation_cm_MTMTST'])(shared_MTMTST2)\n",
        "    lai_MTMTST = Dense(units=params['units_lai_MTMTST'], activation=params['activation_lai_MTMTST'])(shared_MTMTST2)\n",
        "\n",
        "    #this is a just a final output layer needed for the whole thing to work\n",
        "    cab_out = Dense(1, activation='linear', name=\"cab\")(cab_MTMTST)\n",
        "    cw_out  = Dense(1, activation='linear', name=\"cw\")(cw_MTMTST)\n",
        "    cm_out  = Dense(1, activation='linear', name=\"cm\")(cm_MTMTST)\n",
        "    lai_out = Dense(1, activation='linear', name=\"lai\")(lai_MTMTST)\n",
        "\n",
        "    mdl = Model(inputs=inputs, outputs=[cab_out,cw_out,cm_out,lai_out])\n",
        "    mdl.compile(loss='mean_squared_error', optimizer=params['optimizer_MTMTST'], metrics=['accuracy'])\n",
        "\n",
        "  if struct == 'MTMTSTST':\n",
        "    shared_MTMTSTST = Dense(units=params['units_shared_MTMTSTST'],activation=params['activation_MTMTSTST'],name='Shared1')(inputs)\n",
        "    shared_MTMTSTST2 = Dense(units=params['units_shared_MTMTSTST2'],activation=params['activation_MTMTSTST2'],name='Shared2')(shared_MTMTSTST)\n",
        "\n",
        "    cab_MTMTSTST = Dense(units=params['units_cab_MTMTSTST1'], activation=params['activation_cab_MTMTSTST1'],name='ST1_Cab')(shared_MTMTSTST2)\n",
        "    cw_MTMTSTST  = Dense(units=params['units_cw_MTMTSTST1'],  activation=params['activation_cw_MTMTSTST1'],name='ST1_Cw')(shared_MTMTSTST2)\n",
        "    cm_MTMTSTST  = Dense(units=params['units_cm_MTMTSTST1'],  activation=params['activation_cm_MTMTSTST1'],name='ST1_Cm')(shared_MTMTSTST2)\n",
        "    lai_MTMTSTST = Dense(units=params['units_lai_MTMTSTST1'], activation=params['activation_lai_MTMTSTST1'],name='ST1_LAI')(shared_MTMTSTST2)\n",
        "\n",
        "    cab_MTMTSTST2 = Dense(units=params['units_cab_MTMTSTST2'], activation=params['activation_cab_MTMTSTST2'],name='ST2_cab')(cab_MTMTSTST)\n",
        "    cw_MTMTSTST2  = Dense(units=params['units_cw_MTMTSTST2'],  activation=params['activation_cw_MTMTSTST2'],name='ST2_cw')(cw_MTMTSTST)\n",
        "    cm_MTMTSTST2  = Dense(units=params['units_cm_MTMTSTST2'],  activation=params['activation_cm_MTMTSTST2'],name='ST2_cm')(cm_MTMTSTST)\n",
        "    lai_MTMTSTST2 = Dense(units=params['units_lai_MTMTSTST2'], activation=params['activation_lai_MTMTSTST2'],name='ST2_LAI')(lai_MTMTSTST)\n",
        "\n",
        "    #this is a just a final output layer needed for the whole thing to work\n",
        "    cab_out = Dense(1, activation='linear',name=\"cab_out\")(cab_MTMTSTST2)\n",
        "    cw_out  = Dense(1, activation='linear',name=\"cw_out\")(cw_MTMTSTST2)\n",
        "    cm_out  = Dense(1, activation='linear',name=\"cm_out\")(cm_MTMTSTST2)\n",
        "    lai_out = Dense(1, activation='linear',name=\"lai_out\")(lai_MTMTSTST2)\n",
        "\n",
        "    mdl = Model(inputs=inputs, outputs=[cab_out,cw_out,cm_out,lai_out])\n",
        "    mdl.compile(loss='mean_squared_error', optimizer=params['optimizer_MTMTSTST'], metrics=['mean_squared_error'])\n",
        "\n",
        "  return mdl\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVSoZPGLzY6X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "25126751-623f-432e-d739-304b4613ac16"
      },
      "source": [
        "#testing the calls\n",
        "mtn_opt = mt_ann(param_dict_mtn)\n",
        "ann_opt = st_ann(param_dict_st_ann)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d69H68jnSHkr"
      },
      "source": [
        "# Step 4.3: Trait-space\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ly-dqvfSJ7m"
      },
      "source": [
        "n_traits=4 #I will test on 4 varying traits: cab, car, cw,cm,lai\n",
        "\n",
        "#generating a LHS hypercube (it uses a 0 to 1 interval that can be used as a multiplier against the different traits)\n",
        "np.random.seed(0)\n",
        "\n",
        "\n",
        "#the values here are 1 less than the max so i can add a value later to it\n",
        "#max_n=1 \n",
        "max_cab=121. #add 1\n",
        "#max_car=44. #add 1\n",
        "#max_cbrown= 9.99 #add 0.01\n",
        "max_cw=0.008 #add 0.001 \n",
        "max_cm=0.008 #0.001\n",
        "max_lai = 9.9 #add 0.1\n",
        "\n",
        "min_cab = 10.\n",
        "min_cw = 0.002\n",
        "min_cm = 0.002\n",
        "min_lai = .5\n",
        "\n",
        "#labelling\n",
        "label_names = [\"cab\",\"cw\",\"cm\",\"lai\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE9Wnu9vPvDG"
      },
      "source": [
        "# Step 5: setting up the Loop\n",
        "\n",
        "\n",
        "Noise levels\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD8JUReDPrVT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "87050834-16a2-4a1e-fea4-45270aabbc2d"
      },
      "source": [
        "#for a more intuitive call\n",
        "# def range1(start, end,step):\n",
        "#   start=start/100\n",
        "#   end=end/100\n",
        "#   step=step/100\n",
        "#   return range(start, end+1,step)\n",
        "np.linspace(1,10,11)\n",
        "\n",
        "#not intuitive but im tired\n",
        "#noise_level_range = np.arange(0.01, .50, 0.04,)\n",
        "#noise_level_range = np.append(noise_level_range,noise_level_range[0]) #this hack works\n",
        "\n",
        "# noise_level_range = [0.01,0.03,0.05,0.1,\n",
        "#                      0.15,0.2,0.25,\n",
        "#                      0.3,0.4,0.5]\n",
        "# noise_level_range = [0.1,\n",
        "#                      0.15,0.2,0.25,\n",
        "#                      0.3,0.4,0.5]\n",
        "\n",
        "noise_level_range = [0.25,0.3,0.4,0.5]\n",
        "\n",
        "noise_level_range = [0.5]\n",
        "#for i in noise_level_range:\n",
        "#  print(i)\n",
        "print(\"Looping for the following noise values:\",noise_level_range)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looping for the following noise values: [0.5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxNDNwe1zqUF"
      },
      "source": [
        "from datetime import datetime\n",
        "from pytz import timezone    \n",
        "\n",
        "#to have a sense of the time it takes\n",
        "berlin_time = timezone('Europe/Berlin')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKR5YZ4XC42T"
      },
      "source": [
        "creating the trait table - Warning only run once"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWdxiDw7BbjT"
      },
      "source": [
        "\n",
        "#minimium samples\n",
        "n_samples = 2000\n",
        "#generating the trait table once\n",
        "from numpy import savetxt,loadtxt\n",
        "\n",
        "# LHS_train = lhsmdu.createRandomStandardUniformMatrix(n_traits,n_samples)\n",
        "# pd_trait = pd.DataFrame.transpose(pd.DataFrame(LHS_train))\n",
        "# pd_trait.columns = [\"cab\",\"cw\",\"cm\",\"lai\"]\n",
        "\n",
        "# pd_trait[\"cab\"]=pd_trait[\"cab\"]*max_cab+min_cab\n",
        "# pd_trait[\"cw\"] =pd_trait[\"cw\"] *max_cw+min_cw\n",
        "# pd_trait[\"cm\"] =pd_trait[\"cm\"] *max_cm+min_cw\n",
        "# pd_trait[\"lai\"]=pd_trait[\"lai\"]*max_lai+min_lai\n",
        "\n",
        "# #lets also create a numpy object for the traits - this is for ease of operation later\n",
        "# np_trait = pd_trait.iloc[:,:].values\n",
        "\n",
        "# #until here its instantaneous\n",
        "\n",
        "# #now we first generate the date in hyperspectral, convolute it to S2 while iterating through the entire given trait space\n",
        "# print(\"Generating the spectra: \" + datetime.now(berlin_time).strftime('%Y-%m-%d %H:%M:%S'))\n",
        "# np_spect = Gen_spectra_data(pd_trait)[:,[1,2,3,4,5,6,8,11,12]]\n",
        "# print(\"Finished at: \"+ datetime.now(berlin_time).strftime('%Y-%m-%d %H:%M:%S'))\n",
        "\n",
        "# #saving data\n",
        "# savetxt(\"/content/drive/My Drive/RTM_Inversion/Noise/Run_04/np_spect.csv\",np_spect,delimiter=';')\n",
        "# savetxt(\"/content/drive/My Drive/RTM_Inversion/Noise/Run_04/np_trait.csv\",np_trait,delimiter=';')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5njgF6XKyRq"
      },
      "source": [
        "# Main function loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lG1Y5HJBQQ2L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "61dccf66-dc83-4bea-e0f7-4fd71c6b3f88"
      },
      "source": [
        "### storing the dataset\n",
        "\n",
        "#######################################################\n",
        "#BEWARE - > Check the paths for file saving carefully\n",
        "########################################################\n",
        "\n",
        "#this is for the training dataset\n",
        "df_metrics = pd.DataFrame(columns=column_names)\n",
        "#this is for the out of bag\n",
        "df_metrics_valid = pd.DataFrame(columns=column_names)\n",
        "\n",
        "#a path to store the models \n",
        "#path2folder = \"/content/drive/My Drive/DSRP_Lunch_outputs/Models/\"\n",
        "#path2folder = \"/content/drive/My Drive/RTM_Inversion/Noise/Run_01/Models\"\n",
        "#path2folder = \"/content/drive/My Drive/RTM_Inversion/Noise/Run_03/Models/\"\n",
        "path2folder_tables = \"/content/drive/My Drive/RTM_Inversion/Noise/Run_04/Tables/\"\n",
        "\n",
        "\n",
        "#setting epochs\n",
        "number_epochs=600\n",
        "\n",
        "#this command is enough to set u the k-fold\n",
        "fold_nr=10\n",
        "kf = KFold(n_splits=fold_nr,shuffle=True,random_state=0) # Define the split \n",
        "\n",
        "\n",
        "print(\"Looping for the following noise values:\",noise_level_range)\n",
        "\n",
        "for j in noise_level_range:\n",
        "\n",
        "  #this section generates trait space - careful with the column names as this is significant later\n",
        "\n",
        "  np_spect = loadtxt(\"/content/drive/My Drive/RTM_Inversion/Noise/Run_04/np_spect.csv\",delimiter=';')\n",
        "  np_trait = loadtxt(\"/content/drive/My Drive/RTM_Inversion/Noise/Run_04/np_trait.csv\",delimiter=';')\n",
        "\n",
        "\n",
        "  print(\"Adding the noise of \",j*100,\"% to training data: \"+ datetime.now(berlin_time).strftime('%Y-%m-%d %H:%M:%S'))\n",
        "  np_spect_comb = np.apply_along_axis(combined_noise,1,np_spect,sigma=j)\n",
        "  np_spect_inve = np.apply_along_axis(inverted_noise,1,np_spect,sigma=j)\n",
        "  print(\"Finished at: \"+ datetime.now(berlin_time).strftime('%Y-%m-%d %H:%M:%S'))\n",
        "\n",
        "\n",
        "\n",
        "  #now from these generated samples, we take out 10% of the points (aproximately, sometimes its a decimal more, depends on the number)\n",
        "  #creating row identifiers\n",
        "  #this gets the length for the entire list\n",
        "  index = list(range(len(np_spect)))\n",
        "\n",
        "  index10 = rdm.sample(index,math.ceil(len(index)*.1)) #randomly selects 10% of te data (aproximately)\n",
        "  index90 = [x for x in index if x not in index10] #makes a list set wit the reamining\n",
        "  #once we have the list of point, we can subset\n",
        "  \n",
        "  #combined noise\n",
        "  vl_spect_df_comb = np_spect_comb[index10,]\n",
        "  tr_spect_df_comb = np_spect_comb[index90,]\n",
        "\n",
        "  #inverted noise\n",
        "  vl_spect_df_inve = np_spect_inve[index10,]\n",
        "  tr_spect_df_inve = np_spect_inve[index90,]\n",
        "\n",
        "  #traits\n",
        "  vl_trait_df = np_trait[index10,]\n",
        "  tr_trait_df = np_trait[index90,]\n",
        "\n",
        "  #this is a function to re-scale inputs for the neural networks\n",
        "  scaler = MinMaxScaler()\n",
        "\n",
        "  #print(\"Of which, 10% were taken as out-of-bag:\", len(vl_trait_df))\n",
        "\n",
        "  #now we have to iterate per fold, train and store the prediction\n",
        "  k=1\n",
        "  for train_index, test_index in kf.split(tr_trait_df):\n",
        "\n",
        "    print(\"Calculating fold \",k,\"of\",fold_nr)\n",
        "\n",
        "    ################## initializing the model\n",
        "    rfr_ml_comb = rfr_call()\n",
        "    rfr_ml_inve = rfr_call()\n",
        "\n",
        "    gpr_ml_comb = gpr_call(j)\n",
        "    gpr_ml_inve = gpr_call(j)\n",
        "\n",
        "    ann_comb = st_ann(param_dict_st_ann)\n",
        "    ann_inve = st_ann(param_dict_st_ann)\n",
        "\n",
        "    model_comb = mt_ann(param_dict_mtn)\n",
        "    model_inve = mt_ann(param_dict_mtn)\n",
        "\n",
        "    #######################################################################################################################################\n",
        "    ################################ COMBINED ERROR HERE ##################################################################################\n",
        "    #######################################################################################################################################\n",
        "\n",
        "    X_train_comb, X_test_comb = tr_spect_df_comb[train_index], tr_spect_df_comb[test_index]\n",
        "\n",
        "    #trait data\n",
        "    Y_train, Y_test = tr_trait_df[train_index], tr_trait_df[test_index]\n",
        "\n",
        "    #RFR - Training\n",
        "    print(\"Training the RFR (combined noise) @\" + datetime.now(berlin_time).strftime('%Y-%m-%d %H:%M:%S'))\n",
        "    rfr_ml_comb.fit(X_train_comb,Y_train)\n",
        "    print(\"Finished @\" + datetime.now(berlin_time).strftime('%Y-%m-%d %H:%M:%S'))\n",
        "\n",
        "    #GPR - Training \n",
        "    print(\"Training the gaussian processes (combined noise) @\" + datetime.now(berlin_time).strftime('%Y-%m-%d %H:%M:%S'))\n",
        "    gpr_ml_comb.fit(X_train_comb,Y_train)\n",
        "    print(\"Finished @\" + datetime.now(berlin_time).strftime('%Y-%m-%d %H:%M:%S'))\n",
        "\n",
        "    #normalizing the values\n",
        "    scaler.fit(Y_train)\n",
        "    Y_train_norm = scaler.transform(Y_train)\n",
        "\n",
        "    #optimized neural network training\n",
        "    print(\"Training the neural network (combined noise) @\" + datetime.now(berlin_time).strftime('%Y-%m-%d %H:%M:%S'))\n",
        "    ann_comb.fit(X_train_comb,Y_train_norm,epochs=number_epochs,verbose=0)\n",
        "    print(\"Finished @\" + datetime.now(berlin_time).strftime('%Y-%m-%d %H:%M:%S'))\n",
        "\n",
        "    #optimized Multi task ANN training\n",
        "    cab_train = Y_train_norm[:,0]\n",
        "    cw_train = Y_train_norm[:,1]\n",
        "    cm_train = Y_train_norm[:,2]\n",
        "    lai_train = Y_train_norm[:,3]\n",
        "    print(\"Training the multi task neural network (combined noise) @ \" + datetime.now(berlin_time).strftime('%Y-%m-%d %H:%M:%S'))\n",
        "    model_comb.fit(X_train_comb,[cab_train,cw_train,cm_train,lai_train],epochs=number_epochs,verbose=0)#hides spam on the output\n",
        "    print(\"Finished @ \" + datetime.now(berlin_time).strftime('%Y-%m-%d %H:%M:%S'))\n",
        "\n",
        "    #Prediction on training fold\n",
        "    y_rfr_comb = rfr_ml_comb.predict(X_test_comb)\n",
        "    y_gpr_comb = gpr_ml_comb.predict(X_test_comb)\n",
        "    y_ann_comb = scaler.inverse_transform(ann_comb.predict(X_test_comb))\n",
        "\n",
        "    #the multi task is a bit more complicated\n",
        "    Y_mtn_comb = model_comb.predict(X_test_comb)\n",
        "    cab_pred_comb = Y_mtn_comb[0]\n",
        "    cw_pred_comb  = Y_mtn_comb[1]\n",
        "    cm_pred_comb  = Y_mtn_comb[2]\n",
        "    lai_pred_comb = Y_mtn_comb[3]\n",
        "    \n",
        "    np_Y_ann_pred_comb = np.array(cab_pred_comb)\n",
        "    np_Y_ann_pred_comb = np.hstack((np_Y_ann_pred_comb,cw_pred_comb))\n",
        "    np_Y_ann_pred_comb = np.hstack((np_Y_ann_pred_comb,cm_pred_comb))\n",
        "    np_Y_ann_pred_comb = np.hstack((np_Y_ann_pred_comb,lai_pred_comb))\n",
        "\n",
        "    y_mtn_comb = scaler.inverse_transform(np_Y_ann_pred_comb)\n",
        "\n",
        "    #prediction on out of bag\n",
        "    y_rfr_valid_comb = rfr_ml_comb.predict(vl_spect_df_comb)\n",
        "    y_gpr_valid_comb = gpr_ml_comb.predict(vl_spect_df_comb)\n",
        "    y_ann_valid_comb = scaler.inverse_transform(ann_comb.predict(vl_spect_df_comb))\n",
        "\n",
        "    Y_mtn_valid_comb = model_comb.predict(vl_spect_df_comb)\n",
        "    cab_pred_valid_comb = Y_mtn_valid_comb[0]\n",
        "    cw_pred_valid_comb  = Y_mtn_valid_comb[1]\n",
        "    cm_pred_valid_comb  = Y_mtn_valid_comb[2]\n",
        "    lai_pred_valid_comb = Y_mtn_valid_comb[3]\n",
        "    np_Y_ann_pred_valid_comb = np.array(cab_pred_valid_comb)\n",
        "    np_Y_ann_pred_valid_comb = np.hstack((np_Y_ann_pred_valid_comb,cw_pred_valid_comb))\n",
        "    np_Y_ann_pred_valid_comb = np.hstack((np_Y_ann_pred_valid_comb,cm_pred_valid_comb))\n",
        "    np_Y_ann_pred_valid_comb = np.hstack((np_Y_ann_pred_valid_comb,lai_pred_valid_comb))\n",
        "\n",
        "    y_mtn_valid_comb = scaler.inverse_transform(np_Y_ann_pred_valid_comb)\n",
        "\n",
        "\n",
        "    #setting the model filenames\n",
        "    #zfill is a handy function to standardize the output, i love it!\n",
        "    #gpr_ml_modelname = path2folder+\"GPR\"+\"_S\"+str(j).zfill(5)+\"_K\"+str(k).zfill(3)+\".pkl\"\n",
        "    #mtn_ml_modelname = path2folder+\"MTN\"+\"_S\"+str(j).zfill(5)+\"_K\"+str(k).zfill(3)+\".h5\"\n",
        "\n",
        "    #COMMENT HERE IF YOU DO NOT WANT TO SAVE THE MODELS -> old code section needs change\n",
        "    #saving the models\n",
        "    #sklearn are pickles while keras are h5 files\n",
        "    #not saving random forest for now since i am pretty sure it wont be good but it is a big file...\n",
        "    #with open(rfr_ml_modelname, 'wb') as file:\n",
        "    #  pickle.dump(rfr_ml, file)\n",
        "\n",
        "    #with open(gpr_ml_modelname, 'wb') as file:\n",
        "    #  pickle.dump(gpr_ml, file)\n",
        "\n",
        "    #ann_ml.save(ann_ml_modelname)\n",
        "    #model.save(mtn_ml_modelname)\n",
        "      \n",
        "    #######################################################################################################################################\n",
        "    ################################ COMBINED ERROR STOPS HERE ############################################################################\n",
        "    #######################################################################################################################################\n",
        "\n",
        "    #######################################################################################################################################\n",
        "    ################################ INVERTED ERROR HERE ##################################################################################\n",
        "    #######################################################################################################################################\n",
        "\n",
        "    X_train_inve, X_test_inve = tr_spect_df_inve[train_index], tr_spect_df_inve[test_index]\n",
        "\n",
        "    #trait data\n",
        "    Y_train, Y_test = tr_trait_df[train_index], tr_trait_df[test_index]\n",
        "\n",
        "    #RFR - Training\n",
        "    print(\"Training the RFR (inverted) @\" + datetime.now(berlin_time).strftime('%Y-%m-%d %H:%M:%S'))\n",
        "    rfr_ml_inve.fit(X_train_inve,Y_train)\n",
        "    print(\"Finished @ \" + datetime.now(berlin_time).strftime('%Y-%m-%d %H:%M:%S'))\n",
        "\n",
        "    #GPR - Training \n",
        "    print(\"Training the gaussian processes (inverted) @\" + datetime.now(berlin_time).strftime('%Y-%m-%d %H:%M:%S'))\n",
        "    gpr_ml_inve.fit(X_train_inve,Y_train)\n",
        "    print(\"Finished @\" + datetime.now(berlin_time).strftime('%Y-%m-%d %H:%M:%S'))\n",
        "\n",
        "    #normalizing again\n",
        "    scaler.fit(Y_train)\n",
        "    Y_train_norm = scaler.transform(Y_train)\n",
        "\n",
        "    #optimized neural network training\n",
        "    print(\"Training the neural network (inverted) @\" + datetime.now(berlin_time).strftime('%Y-%m-%d %H:%M:%S'))\n",
        "    ann_inve.fit(X_train_inve,Y_train_norm,epochs=number_epochs,verbose=0)\n",
        "    print(\"Finished @\" + datetime.now(berlin_time).strftime('%Y-%m-%d %H:%M:%S'))\n",
        "\n",
        "    #optimized multi task\n",
        "    cab_train = Y_train_norm[:,0]\n",
        "    cw_train = Y_train_norm[:,1]\n",
        "    cm_train = Y_train_norm[:,2]\n",
        "    lai_train = Y_train_norm[:,3]\n",
        "    print(\"Training the multi task neural network (inverted) @ \" + datetime.now(berlin_time).strftime('%Y-%m-%d %H:%M:%S'))\n",
        "    model_inve.fit(X_train_inve,[cab_train,cw_train,cm_train,lai_train],epochs=number_epochs,verbose=0)#hides spam on the output\n",
        "    print(\"Finished @ \" + datetime.now(berlin_time).strftime('%Y-%m-%d %H:%M:%S'))\n",
        "\n",
        "    #Prediction on training fold\n",
        "    y_rfr_inve = rfr_ml_inve.predict(X_test_inve)   \n",
        "    y_gpr_inve = gpr_ml_inve.predict(X_test_inve)\n",
        "    y_ann_inve = scaler.inverse_transform(ann_inve.predict(X_test_inve)) \n",
        "\n",
        "    #the multi task is a bit more complicated\n",
        "    Y_mtn_inve = model_inve.predict(X_test_inve)\n",
        "    cab_pred_inve = Y_mtn_inve[0]\n",
        "    cw_pred_inve  = Y_mtn_inve[1]\n",
        "    cm_pred_inve  = Y_mtn_inve[2]\n",
        "    lai_pred_inve = Y_mtn_inve[3]\n",
        "    \n",
        "    np_Y_ann_pred_inve = np.array(cab_pred_inve)\n",
        "    np_Y_ann_pred_inve = np.hstack((np_Y_ann_pred_inve,cw_pred_inve))\n",
        "    np_Y_ann_pred_inve = np.hstack((np_Y_ann_pred_inve,cm_pred_inve))\n",
        "    np_Y_ann_pred_inve = np.hstack((np_Y_ann_pred_inve,lai_pred_inve))\n",
        "\n",
        "    y_mtn_inve = scaler.inverse_transform(np_Y_ann_pred_inve)\n",
        "\n",
        "    #prediction on out of bag\n",
        "    #inverted\n",
        "    y_rfr_valid_inve = rfr_ml_comb.predict(vl_spect_df_inve)   \n",
        "    y_gpr_valid_inve = gpr_ml_inve.predict(vl_spect_df_inve)\n",
        "    y_ann_valid_inve = scaler.inverse_transform(ann_comb.predict(vl_spect_df_inve)) \n",
        "\n",
        "    Y_mtn_valid_inve = model_inve.predict(vl_spect_df_inve)\n",
        "    cab_pred_valid_inve = Y_mtn_valid_inve[0]\n",
        "    cw_pred_valid_inve  = Y_mtn_valid_inve[1]\n",
        "    cm_pred_valid_inve  = Y_mtn_valid_inve[2]\n",
        "    lai_pred_valid_inve = Y_mtn_valid_inve[3]\n",
        "    np_Y_ann_pred_valid_inve = np.array(cab_pred_valid_inve)\n",
        "    np_Y_ann_pred_valid_inve = np.hstack((np_Y_ann_pred_valid_inve,cw_pred_valid_inve))\n",
        "    np_Y_ann_pred_valid_inve = np.hstack((np_Y_ann_pred_valid_inve,cm_pred_valid_inve))\n",
        "    np_Y_ann_pred_valid_inve = np.hstack((np_Y_ann_pred_valid_inve,lai_pred_valid_inve))\n",
        "\n",
        "    y_mtn_valid_inve = scaler.inverse_transform(np_Y_ann_pred_valid_inve)\n",
        "\n",
        "\n",
        "    #setting the model filenames\n",
        "    #zfill is a handy function to standardize the output, i love it!\n",
        "    #gpr_ml_modelname = path2folder+\"GPR\"+\"_S\"+str(j).zfill(5)+\"_K\"+str(k).zfill(3)+\".pkl\"\n",
        "    #mtn_ml_modelname = path2folder+\"MTN\"+\"_S\"+str(j).zfill(5)+\"_K\"+str(k).zfill(3)+\".h5\"\n",
        "\n",
        "    #COMMENT HERE IF YOU DO NOT WANT TO SAVE THE MODELS -> old code section needs change\n",
        "    #saving the models\n",
        "    #sklearn are pickles while keras are h5 files\n",
        "    #not saving random forest for now since i am pretty sure it wont be good but it is a big file...\n",
        "    #with open(rfr_ml_modelname, 'wb') as file:\n",
        "    #  pickle.dump(rfr_ml, file)\n",
        "\n",
        "    #with open(gpr_ml_modelname, 'wb') as file:\n",
        "    #  pickle.dump(gpr_ml, file)\n",
        "\n",
        "    #ann_ml.save(ann_ml_modelname)\n",
        "    #model.save(mtn_ml_modelname)\n",
        "\n",
        "\n",
        "    #######################################################################################################################################\n",
        "    ################################ INVERTED ERROR STOPS HERE ############################################################################\n",
        "    #######################################################################################################################################\n",
        "\n",
        "    #the next section stores all the results\n",
        "    for i in range(n_traits):\n",
        "\n",
        "      #shared info:\n",
        "      out_bag_samples = len(vl_spect_df_comb)\n",
        "      tr_samples = len(X_train_comb)\n",
        "      vl_samples = len(X_test_comb)\n",
        "      var_name = label_names[i]\n",
        "      tr_Y = Y_test[:,i]\n",
        "      ob_Y = vl_trait_df[:,i]\n",
        "\n",
        "\n",
        "      #this stores the training accuraccy metrics\n",
        "      #print(i)\n",
        "      rfr_temp_list_comb = calc_metrics(\"RFR\",j,\"Combined\",\n",
        "                                        out_bag_samples,tr_samples,vl_samples,var_name,k,\n",
        "                                        tr_Y,y_rfr_comb[:,i],\"Not stored\")\n",
        "\n",
        "      gpr_temp_list_comb = calc_metrics(\"GPR\",j,\"Combined\",\n",
        "                                        out_bag_samples,tr_samples,vl_samples,var_name,k,\n",
        "                                        tr_Y,y_gpr_comb[:,i],\"Not stored\")\n",
        "\n",
        "      ann_temp_list_comb = calc_metrics(\"ANN\",j,\"Combined\",\n",
        "                                        out_bag_samples,tr_samples,vl_samples,var_name,k,\n",
        "                                        tr_Y,y_ann_comb[:,i],\"Not stored\")\n",
        "      \n",
        "      mtn_temp_list_comb = calc_metrics(\"MTN\",j,\"Combined\",\n",
        "                                        out_bag_samples,tr_samples,vl_samples,var_name,k,\n",
        "                                        tr_Y,y_mtn_comb[:,i],\"Not stored\")\n",
        "\n",
        "      #This stores the metrics against the out of bag list\n",
        "      rfr_temp_list_valid_comb = calc_metrics(\"RFR\",j,\"Combined\",\n",
        "                                              out_bag_samples,tr_samples,vl_samples,var_name,k,\n",
        "                                              ob_Y,y_rfr_valid_comb[:,i],\"Not stored\")\n",
        "\n",
        "      gpr_temp_list_valid_comb = calc_metrics(\"GPR\",j,\"Combined\",\n",
        "                                              out_bag_samples,tr_samples,vl_samples,var_name,k,\n",
        "                                              ob_Y,y_gpr_valid_comb[:,i],\"Not stored\")\n",
        "      \n",
        "      ann_temp_list_valid_comb = calc_metrics(\"ANN\",j,\"Combined\",\n",
        "                                              out_bag_samples,tr_samples,vl_samples,var_name,k,\n",
        "                                              ob_Y,y_ann_valid_comb[:,i],\"Not stored\")\n",
        "      \n",
        "      mtn_temp_list_valid_comb = calc_metrics(\"MTN\",j,\"Combined\",\n",
        "                                              out_bag_samples,tr_samples,vl_samples,var_name,k,\n",
        "                                              ob_Y,y_mtn_valid_comb[:,i],\"Not stored\")\n",
        "\n",
        "\n",
        "      ######################### inverted noise #########################\n",
        "      rfr_temp_list_inve = calc_metrics(\"RFR\",j,\"Inverted\",\n",
        "                                        out_bag_samples,tr_samples,vl_samples,var_name,k,\n",
        "                                        tr_Y,y_rfr_inve[:,i],\"Not stored\")\n",
        "\n",
        "      gpr_temp_list_inve = calc_metrics(\"GPR\",j,\"Inverted\",\n",
        "                                        out_bag_samples,tr_samples,vl_samples,var_name,k,\n",
        "                                        tr_Y,y_gpr_inve[:,i],\"Not stored\")\n",
        "      \n",
        "      ann_temp_list_inve = calc_metrics(\"ANN\",j,\"Inverted\",\n",
        "                                        out_bag_samples,tr_samples,vl_samples,var_name,k,\n",
        "                                        tr_Y,y_ann_inve[:,i],\"Not stored\")\n",
        "    \n",
        "      mtn_temp_list_inve = calc_metrics(\"MTN\",j,\"Inverted\",\n",
        "                                        out_bag_samples,tr_samples,vl_samples,var_name,k,\n",
        "                                        tr_Y,y_mtn_inve[:,i],\"Not stored\")\n",
        "\n",
        "      #This stores the metrics against the out of bag list\n",
        "      rfr_temp_list_valid_inve = calc_metrics(\"RFR\",j,\"Inverted\",\n",
        "                                              out_bag_samples,tr_samples,vl_samples,var_name,k,\n",
        "                                              ob_Y,y_rfr_valid_inve[:,i],\"Not stored\")\n",
        "\n",
        "      gpr_temp_list_valid_inve = calc_metrics(\"GPR\",j,\"Inverted\",\n",
        "                                              out_bag_samples,tr_samples,vl_samples,var_name,k,\n",
        "                                              ob_Y,y_gpr_valid_inve[:,i],\"Not stored\")\n",
        "\n",
        "      ann_temp_list_valid_inve = calc_metrics(\"ANN\",j,\"Inverted\",\n",
        "                                              out_bag_samples,tr_samples,vl_samples,var_name,k,\n",
        "                                              ob_Y,y_ann_valid_inve[:,i],\"Not stored\")\n",
        "                                                    \n",
        "      mtn_temp_list_valid_inve = calc_metrics(\"MTN\",j,\"Inverted\",\n",
        "                                              out_bag_samples,tr_samples,vl_samples,var_name,k,\n",
        "                                              ob_Y,y_mtn_valid_inve[:,i],\"Not stored\")\n",
        "\n",
        "\n",
        "      #appending to the dataframe (training error)\n",
        "      df_metrics = df_metrics.append(rfr_temp_list_comb,ignore_index=True)\n",
        "      df_metrics = df_metrics.append(gpr_temp_list_comb,ignore_index=True)\n",
        "      df_metrics = df_metrics.append(ann_temp_list_comb,ignore_index=True)      \n",
        "      df_metrics = df_metrics.append(mtn_temp_list_comb,ignore_index=True)\n",
        "\n",
        "      df_metrics = df_metrics.append(rfr_temp_list_inve,ignore_index=True)\n",
        "      df_metrics = df_metrics.append(gpr_temp_list_inve,ignore_index=True)\n",
        "      df_metrics = df_metrics.append(ann_temp_list_inve,ignore_index=True)\n",
        "      df_metrics = df_metrics.append(mtn_temp_list_inve,ignore_index=True)\n",
        "\n",
        "      #appending to the dataframe (out of bag error)\n",
        "      df_metrics_valid = df_metrics_valid.append(rfr_temp_list_valid_comb,ignore_index=True)\n",
        "      df_metrics_valid = df_metrics_valid.append(gpr_temp_list_valid_comb,ignore_index=True)\n",
        "      df_metrics_valid = df_metrics_valid.append(ann_temp_list_valid_comb,ignore_index=True)\n",
        "      df_metrics_valid = df_metrics_valid.append(mtn_temp_list_valid_comb,ignore_index=True)\n",
        "\n",
        "      df_metrics_valid = df_metrics_valid.append(rfr_temp_list_valid_inve,ignore_index=True)\n",
        "      df_metrics_valid = df_metrics_valid.append(gpr_temp_list_valid_inve,ignore_index=True)\n",
        "      df_metrics_valid = df_metrics_valid.append(ann_temp_list_valid_inve,ignore_index=True)\n",
        "      df_metrics_valid = df_metrics_valid.append(mtn_temp_list_valid_inve,ignore_index=True)   \n",
        "\n",
        "    k=k+1\n",
        "\n",
        "    #saves a temp version of the files\n",
        "    #saving the file to an output\n",
        "  \n",
        "  print(\"Of which\",len(X_train_comb),\"were used for training and\",len(X_test_comb),\"for validation\")\n",
        "\n",
        "  df_metrics.to_csv(path2folder_tables + \"Optim_S2000_Temp_Noise_KFold_TrainingError_csv2_50perc.csv\",sep=\";\",decimal=\",\")\n",
        "  df_metrics_valid.to_csv(path2folder_tables + \"Optim_S2000_Temp_Noise_KFold_OutOfBag_csv2_50perc.csv\",sep=\";\",decimal=\",\")\n",
        "\n",
        "  df_metrics.to_csv(path2folder_tables + \"Optim_S2000_Temp_Noise_KFold_TrainingError_csv1_50perc.csv\",sep=\",\",decimal=\".\")\n",
        "  df_metrics_valid.to_csv(path2folder_tables + \"Optim_S2000_Temp_Noise_KFold_OutOfBag_csv1_50perc.csv\",sep=\",\",decimal=\".\")\n",
        "\n",
        "  print(\"Temporary files saved\")\n",
        "\n",
        "\n",
        "#saving the file to an output\n",
        "df_metrics.to_csv(path2folder_tables + \"Optim_S2000_Noise_KFold_TrainingError_csv2_50perc.csv\",sep=\";\",decimal=\",\")\n",
        "df_metrics_valid.to_csv(path2folder_tables + \"Optim_S2000_Noise_KFold_OutOfBag_csv2_50perc.csv\",sep=\";\",decimal=\",\")\n",
        "\n",
        "df_metrics.to_csv(path2folder_tables + \"Optim_S2000_Noise_KFold_TrainingError_csv1_50perc.csv\",sep=\",\",decimal=\".\")\n",
        "df_metrics_valid.to_csv(path2folder_tables + \"Optim_S2000_Noise_KFold_OutOfBag_csv1_50perc.csv\",sep=\",\",decimal=\".\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looping for the following noise values: [0.5]\n",
            "Adding the noise of  50.0 % to training data: 2020-06-01 17:44:23\n",
            "Finished at: 2020-06-01 17:44:25\n",
            "Calculating fold  1 of 10\n",
            "Training the RFR (combined noise) @2020-06-01 17:44:26\n",
            "Finished @2020-06-01 17:44:34\n",
            "Training the gaussian processes (combined noise) @2020-06-01 17:44:34\n",
            "Finished @2020-06-01 17:44:34\n",
            "Training the neural network (combined noise) @2020-06-01 17:44:34\n",
            "Finished @2020-06-01 17:45:10\n",
            "Training the multi task neural network (combined noise) @ 2020-06-01 17:45:10\n",
            "Finished @ 2020-06-01 17:46:12\n",
            "Training the RFR (inverted) @2020-06-01 17:46:12\n",
            "Finished @ 2020-06-01 17:46:18\n",
            "Training the gaussian processes (inverted) @2020-06-01 17:46:18\n",
            "Finished @2020-06-01 17:46:19\n",
            "Training the neural network (inverted) @2020-06-01 17:46:19\n",
            "Finished @2020-06-01 17:46:55\n",
            "Training the multi task neural network (inverted) @ 2020-06-01 17:46:55\n",
            "Finished @ 2020-06-01 17:48:02\n",
            "Calculating fold  2 of 10\n",
            "Training the RFR (combined noise) @2020-06-01 17:48:04\n",
            "Finished @2020-06-01 17:48:12\n",
            "Training the gaussian processes (combined noise) @2020-06-01 17:48:12\n",
            "Finished @2020-06-01 17:48:12\n",
            "Training the neural network (combined noise) @2020-06-01 17:48:12\n",
            "Finished @2020-06-01 17:48:54\n",
            "Training the multi task neural network (combined noise) @ 2020-06-01 17:48:54\n",
            "Finished @ 2020-06-01 17:50:06\n",
            "Training the RFR (inverted) @2020-06-01 17:50:07\n",
            "Finished @ 2020-06-01 17:50:13\n",
            "Training the gaussian processes (inverted) @2020-06-01 17:50:13\n",
            "Finished @2020-06-01 17:50:13\n",
            "Training the neural network (inverted) @2020-06-01 17:50:13\n",
            "Finished @2020-06-01 17:50:57\n",
            "Training the multi task neural network (inverted) @ 2020-06-01 17:50:57\n",
            "Finished @ 2020-06-01 17:52:14\n",
            "Calculating fold  3 of 10\n",
            "Training the RFR (combined noise) @2020-06-01 17:52:16\n",
            "Finished @2020-06-01 17:52:24\n",
            "Training the gaussian processes (combined noise) @2020-06-01 17:52:24\n",
            "Finished @2020-06-01 17:52:25\n",
            "Training the neural network (combined noise) @2020-06-01 17:52:25\n",
            "Finished @2020-06-01 17:53:13\n",
            "Training the multi task neural network (combined noise) @ 2020-06-01 17:53:13\n",
            "Finished @ 2020-06-01 17:54:35\n",
            "Training the RFR (inverted) @2020-06-01 17:54:36\n",
            "Finished @ 2020-06-01 17:54:42\n",
            "Training the gaussian processes (inverted) @2020-06-01 17:54:42\n",
            "Finished @2020-06-01 17:54:42\n",
            "Training the neural network (inverted) @2020-06-01 17:54:42\n",
            "Finished @2020-06-01 17:55:32\n",
            "Training the multi task neural network (inverted) @ 2020-06-01 17:55:32\n",
            "Finished @ 2020-06-01 17:56:58\n",
            "Calculating fold  4 of 10\n",
            "Training the RFR (combined noise) @2020-06-01 17:57:01\n",
            "Finished @2020-06-01 17:57:09\n",
            "Training the gaussian processes (combined noise) @2020-06-01 17:57:09\n",
            "Finished @2020-06-01 17:57:09\n",
            "Training the neural network (combined noise) @2020-06-01 17:57:09\n",
            "Finished @2020-06-01 17:58:04\n",
            "Training the multi task neural network (combined noise) @ 2020-06-01 17:58:04\n",
            "Finished @ 2020-06-01 17:59:35\n",
            "Training the RFR (inverted) @2020-06-01 17:59:36\n",
            "Finished @ 2020-06-01 17:59:43\n",
            "Training the gaussian processes (inverted) @2020-06-01 17:59:43\n",
            "Finished @2020-06-01 17:59:43\n",
            "Training the neural network (inverted) @2020-06-01 17:59:43\n",
            "Finished @2020-06-01 18:00:40\n",
            "Training the multi task neural network (inverted) @ 2020-06-01 18:00:40\n",
            "Finished @ 2020-06-01 18:02:15\n",
            "Calculating fold  5 of 10\n",
            "Training the RFR (combined noise) @2020-06-01 18:02:19\n",
            "Finished @2020-06-01 18:02:27\n",
            "Training the gaussian processes (combined noise) @2020-06-01 18:02:27\n",
            "Finished @2020-06-01 18:02:27\n",
            "Training the neural network (combined noise) @2020-06-01 18:02:27\n",
            "Finished @2020-06-01 18:03:30\n",
            "Training the multi task neural network (combined noise) @ 2020-06-01 18:03:30\n",
            "Finished @ 2020-06-01 18:05:12\n",
            "Training the RFR (inverted) @2020-06-01 18:05:14\n",
            "Finished @ 2020-06-01 18:05:20\n",
            "Training the gaussian processes (inverted) @2020-06-01 18:05:20\n",
            "Finished @2020-06-01 18:05:21\n",
            "Training the neural network (inverted) @2020-06-01 18:05:21\n",
            "Finished @2020-06-01 18:06:25\n",
            "Training the multi task neural network (inverted) @ 2020-06-01 18:06:25\n",
            "Finished @ 2020-06-01 18:08:09\n",
            "Calculating fold  6 of 10\n",
            "Training the RFR (combined noise) @2020-06-01 18:08:13\n",
            "Finished @2020-06-01 18:08:21\n",
            "Training the gaussian processes (combined noise) @2020-06-01 18:08:21\n",
            "Finished @2020-06-01 18:08:21\n",
            "Training the neural network (combined noise) @2020-06-01 18:08:21\n",
            "Finished @2020-06-01 18:09:30\n",
            "Training the multi task neural network (combined noise) @ 2020-06-01 18:09:30\n",
            "Finished @ 2020-06-01 18:11:20\n",
            "Training the RFR (inverted) @2020-06-01 18:11:23\n",
            "Finished @ 2020-06-01 18:11:29\n",
            "Training the gaussian processes (inverted) @2020-06-01 18:11:29\n",
            "Finished @2020-06-01 18:11:29\n",
            "Training the neural network (inverted) @2020-06-01 18:11:29\n",
            "Finished @2020-06-01 18:12:41\n",
            "Training the multi task neural network (inverted) @ 2020-06-01 18:12:41\n",
            "Finished @ 2020-06-01 18:14:34\n",
            "Calculating fold  7 of 10\n",
            "Training the RFR (combined noise) @2020-06-01 18:14:38\n",
            "Finished @2020-06-01 18:14:46\n",
            "Training the gaussian processes (combined noise) @2020-06-01 18:14:46\n",
            "Finished @2020-06-01 18:14:47\n",
            "Training the neural network (combined noise) @2020-06-01 18:14:47\n",
            "Finished @2020-06-01 18:16:03\n",
            "Training the multi task neural network (combined noise) @ 2020-06-01 18:16:03\n",
            "Finished @ 2020-06-01 18:18:05\n",
            "Training the RFR (inverted) @2020-06-01 18:18:08\n",
            "Finished @ 2020-06-01 18:18:14\n",
            "Training the gaussian processes (inverted) @2020-06-01 18:18:14\n",
            "Finished @2020-06-01 18:18:14\n",
            "Training the neural network (inverted) @2020-06-01 18:18:14\n",
            "Finished @2020-06-01 18:19:33\n",
            "Training the multi task neural network (inverted) @ 2020-06-01 18:19:33\n",
            "Finished @ 2020-06-01 18:21:39\n",
            "Calculating fold  8 of 10\n",
            "Training the RFR (combined noise) @2020-06-01 18:21:44\n",
            "Finished @2020-06-01 18:21:52\n",
            "Training the gaussian processes (combined noise) @2020-06-01 18:21:52\n",
            "Finished @2020-06-01 18:21:53\n",
            "Training the neural network (combined noise) @2020-06-01 18:21:53\n",
            "Finished @2020-06-01 18:23:18\n",
            "Training the multi task neural network (combined noise) @ 2020-06-01 18:23:18\n",
            "Finished @ 2020-06-01 18:25:31\n",
            "Training the RFR (inverted) @2020-06-01 18:25:34\n",
            "Finished @ 2020-06-01 18:25:40\n",
            "Training the gaussian processes (inverted) @2020-06-01 18:25:40\n",
            "Finished @2020-06-01 18:25:41\n",
            "Training the neural network (inverted) @2020-06-01 18:25:41\n",
            "Finished @2020-06-01 18:27:07\n",
            "Training the multi task neural network (inverted) @ 2020-06-01 18:27:07\n",
            "Finished @ 2020-06-01 18:29:22\n",
            "Calculating fold  9 of 10\n",
            "Training the RFR (combined noise) @2020-06-01 18:29:27\n",
            "Finished @2020-06-01 18:29:35\n",
            "Training the gaussian processes (combined noise) @2020-06-01 18:29:35\n",
            "Finished @2020-06-01 18:29:35\n",
            "Training the neural network (combined noise) @2020-06-01 18:29:35\n",
            "Finished @2020-06-01 18:31:06\n",
            "Training the multi task neural network (combined noise) @ 2020-06-01 18:31:06\n",
            "Finished @ 2020-06-01 18:33:31\n",
            "Training the RFR (inverted) @2020-06-01 18:33:35\n",
            "Finished @ 2020-06-01 18:33:41\n",
            "Training the gaussian processes (inverted) @2020-06-01 18:33:41\n",
            "Finished @2020-06-01 18:33:41\n",
            "Training the neural network (inverted) @2020-06-01 18:33:41\n",
            "Finished @2020-06-01 18:35:15\n",
            "Training the multi task neural network (inverted) @ 2020-06-01 18:35:15\n",
            "Finished @ 2020-06-01 18:37:44\n",
            "Calculating fold  10 of 10\n",
            "Training the RFR (combined noise) @2020-06-01 18:37:50\n",
            "Finished @2020-06-01 18:37:58\n",
            "Training the gaussian processes (combined noise) @2020-06-01 18:37:58\n",
            "Finished @2020-06-01 18:37:58\n",
            "Training the neural network (combined noise) @2020-06-01 18:37:58\n",
            "Finished @2020-06-01 18:39:37\n",
            "Training the multi task neural network (combined noise) @ 2020-06-01 18:39:37\n",
            "Finished @ 2020-06-01 18:42:12\n",
            "Training the RFR (inverted) @2020-06-01 18:42:17\n",
            "Finished @ 2020-06-01 18:42:23\n",
            "Training the gaussian processes (inverted) @2020-06-01 18:42:23\n",
            "Finished @2020-06-01 18:42:23\n",
            "Training the neural network (inverted) @2020-06-01 18:42:23\n",
            "Finished @2020-06-01 18:44:07\n",
            "Training the multi task neural network (inverted) @ 2020-06-01 18:44:07\n",
            "Finished @ 2020-06-01 18:46:44\n",
            "Of which 1620 were used for training and 180 for validation\n",
            "Temporary files saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzqWIJCEn_da"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}