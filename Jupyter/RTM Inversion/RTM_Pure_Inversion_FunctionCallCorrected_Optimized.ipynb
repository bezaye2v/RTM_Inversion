{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RTM Pure Inversion_FunctionCallCorrected_Optimized.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY6CZWCJ6CSF"
      },
      "source": [
        "# Step 1: mount the google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnQrdGfy5_DM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "d053079e-9f00-422d-c66c-198f713e0a22"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#Perhaps this step can be skipped by saving directly to the workspace"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh1wecSI6i17"
      },
      "source": [
        "# Step 2: install the packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RW6U50tH6Qh2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "outputId": "e0291f34-eea1-45ef-b88e-b396c40ddbd1"
      },
      "source": [
        "#package instalation\n",
        "\n",
        "#Installing PROSAIL\n",
        "!pip install prosail\n",
        "\n",
        "#latin hypercube stuff\n",
        "#lets try to do a LHS\n",
        "!pip install lhsmdu\n",
        "\n",
        "#this package as a number of functions to deal with hyperspectral data\n",
        "!pip install pysptools"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting prosail\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b5/52/d0c15ab469e8c82bc76a6b6cd614efbc60e43d09d5bacaa349170d229e91/prosail-2.0.5-py3-none-any.whl (149kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from prosail) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from prosail) (1.18.4)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from prosail) (3.6.4)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from prosail) (0.48.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (1.8.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (46.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (19.3.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (8.3.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (1.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (1.12.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->prosail) (0.7.1)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->prosail) (0.31.0)\n",
            "Installing collected packages: prosail\n",
            "Successfully installed prosail-2.0.5\n",
            "Collecting lhsmdu\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/f0/e714a4dae734bcd7228a09d74fff7dc5857dc3311cd72a3e07b09c85d088/lhsmdu-0.1-py3-none-any.whl\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lhsmdu) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lhsmdu) (1.18.4)\n",
            "Installing collected packages: lhsmdu\n",
            "Successfully installed lhsmdu-0.1\n",
            "Collecting pysptools\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/20/cef48129eff2bdcb282279138c09e6f04770a8fdcb3c1bb9a98fe4086d2d/pysptools-0.15.0.tar.gz (8.1MB)\n",
            "\u001b[K     |████████████████████████████████| 8.1MB 2.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pysptools\n",
            "  Building wheel for pysptools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pysptools: filename=pysptools-0.15.0-cp36-none-any.whl size=8133750 sha256=c9704c7d2221fb3158d4336a0441ad98af064578e2adf02f9eb6d50ac2580aad\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/60/be/a6719d91bfa59135201feb034c7069e4146aa576fc0dc9e624\n",
            "Successfully built pysptools\n",
            "Installing collected packages: pysptools\n",
            "Successfully installed pysptools-0.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx43URaVHK2p"
      },
      "source": [
        "#TensorFlow update correction\n",
        "\n",
        "Tensor flow has been updated so we will uninstall the new version and push in the old version which works well on my case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpXTNLTqHDSc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "316c57a7-84ba-4448-f34e-e1ff1b89efbc"
      },
      "source": [
        "#!pip uninstall tensorflow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.2.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow-2.2.0.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jf3PstT3HRMQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692
        },
        "outputId": "d1823979-6ae8-4868-8e03-8dd05f3948d4"
      },
      "source": [
        "#!pip install tensorflow==1.14.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n",
            "\u001b[K     |████████████████████████████████| 109.2MB 92kB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.9.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.8.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.2)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 38.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.29.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.18.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.2.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.3.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.34.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (3.10.0)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 39.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.14.0) (46.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.1.0)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "  Found existing installation: tensorboard 2.2.1\n",
            "    Uninstalling tensorboard-2.2.1:\n",
            "      Successfully uninstalled tensorboard-2.2.1\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suGQqDex6qUW"
      },
      "source": [
        "# Step 3 Import the packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxVwwv0G6pog",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "2b2bc0bd-d5fd-4e80-fd2a-f97ceb54334d"
      },
      "source": [
        "#General purpose: \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "#the beutiful R like data frame\n",
        "import pandas as pd\n",
        "\n",
        "#PROSPECT+SAIL Radiative transfer mode package\n",
        "import prosail\n",
        "\n",
        "#Sampling design package\n",
        "import lhsmdu\n",
        "\n",
        "#a few more stuff for random\n",
        "import random as rdm\n",
        "import math\n",
        "\n",
        "\n",
        "#package to for operations on spectral data\n",
        "import pysptools as sptool \n",
        "from pysptools import distance\n",
        "\n",
        "\n",
        "from sklearn.model_selection import KFold # import KFold\n",
        "\n",
        "#machine learning stuff\n",
        "#NEURAL NETWORK - Keras will be updated soon so this colab will also have to be changed\n",
        "\n",
        "from sklearn.neural_network import MLPRegressor as ANN_reg #this is a simpler neural network package\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Input\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "#ignore the warning for now\n",
        "\n",
        "#Random FOREST\n",
        "# Fitting Random Forest Regression to the dataset \n",
        "# import the regressor \n",
        "from sklearn.ensemble import RandomForestRegressor \n",
        "\n",
        "#Gaussian processes\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import ConstantKernel, Matern, WhiteKernel, ExpSineSquared, RationalQuadratic, RBF\n",
        "\n",
        "#aux functions\n",
        "from sklearn.preprocessing import MinMaxScaler #this is to standardize the input data [not used for now]\n",
        "from sklearn import metrics\n",
        "\n",
        "#for model storing -sklearn\n",
        "import pickle\n",
        "\n",
        "#for model storing keras\n",
        "from keras.models import model_from_json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q16hIsNM6CXE"
      },
      "source": [
        "# Step 4.1: set up of custom auxiliar functions\n",
        "\n",
        "- First we have to set of custom function to facilitate the loop calls\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOCoSTJf7ZNO"
      },
      "source": [
        "def custom_prosail(cab,cw,cm,lai):\n",
        "  import prosail\n",
        "  #default parameters\n",
        "  n= 1.\n",
        "  car=10.\n",
        "  cbrown=0.01\n",
        "  typelidf=1 #this is the default option\n",
        "  lidfa = -1 #leaf angle distribution parameter a and b\n",
        "  lidfb=-0.15\n",
        "  hspot= 0.01 #hotspot parameters - got this from R package https://www.rdocumentation.org/packages/hsdar/versions/0.4.1/topics/PROSAIL\n",
        "  #sun and viewing angle\n",
        "  tts=30. #observation and solar position parameters\n",
        "  tto=10. \n",
        "  psi=0.\n",
        "  #for now i put them by hand but they should be an input of a custom function\n",
        "  #tts=sol_zen #solar zenith angle\n",
        "  #tto=inc_zen #sensor zenith angle\n",
        "  #psi=raa\n",
        "  rho_out = prosail.run_prosail(n,\n",
        "                                 cab,\n",
        "                                 car,\n",
        "                                 cbrown,\n",
        "                                 cw,\n",
        "                                 cm,\n",
        "                                 lai,\n",
        "                                 lidfa,\n",
        "                                 hspot,\n",
        "                                 tts,tto,psi,\n",
        "                                 typelidf, lidfb,\n",
        "                                 prospect_version=\"D\",\n",
        "                                 factor='SDR', \n",
        "                                 rsoil=.5, psoil=.5)\n",
        "  return(rho_out)\n",
        "\n",
        "def Prosail2S2(path2csv,spectra_input):\n",
        "  #importing pandas\n",
        "  import pandas as pd\n",
        "  import numpy\n",
        "  import numpy as np\n",
        "  #upload a S2_Response.csv from https://earth.esa.int/web/sentinel/user-guides/sentinel-2-msi/document-library/-/asset_publisher/Wk0TKajiISaR/content/sentinel-2a-spectral-responses\n",
        "\n",
        "  s2_table = pd.read_csv(path2csv,sep=\";\",decimal=\",\") #check if this is proper, regarding the sep and dec\n",
        "  #chekc which row you are actually extracting\n",
        "\n",
        "  s2_table_sel = s2_table[s2_table['SR_WL'].between(400,2500)] #selects all values between 400 and 2500\n",
        "  spectra_input_df = pd.DataFrame(data=spectra_input,columns=[\"rho\"],index=s2_table_sel.index) #transforms the input array into a pandas df with the column name rho and row.index = to the original input table\n",
        "\n",
        "  \n",
        "  rho_s2 = s2_table_sel.multiply(spectra_input_df['rho'],axis=\"index\") #calculates the numerator\n",
        "  w_band_sum = s2_table_sel.sum(axis=0,skipna = True) #calculates the denominator\n",
        "\n",
        "  output = (rho_s2.sum(axis=0)/w_band_sum).rename_axis(\"ID\").values #runs the weighted mean and converts the output to a numpy array\n",
        "\n",
        "  return output[1:] #removes the first value because it represents the wavelength column\n",
        "\n",
        "#please LOAD THTE FILE NOW\n",
        "filepath=\"/content/drive/My Drive/S2_Response.csv\"\n",
        "#filepath=\"/content/S2_Responses_S2B.csv\"\n",
        "#filepath=\"/content/drive/My Drive/S2_Response.csv\"\n",
        "\n",
        "\n",
        "def Gen_spectra_data(traits):\n",
        "  k = 1\n",
        "  #pd_train_traits=traits\n",
        "  #print(range(len(traits)))\n",
        "  for i in range(len(traits)):\n",
        "    #n_t = pd_train_traits[\"n\"][i]\n",
        "    cab_t = traits[\"cab\"][i]\n",
        "    #car_t = pd_train_traits[\"car\"][i]\n",
        "    #cbrown_t = pd_train_traits[\"cbrown\"][i]\n",
        "    cw_t = traits[\"cw\"][i]\n",
        "    cm_t = traits[\"cm\"][i]\n",
        "    lai_t = traits[\"lai\"][i]\n",
        "\n",
        "    if k == 1:\n",
        "      tr_rho_s = custom_prosail(cab_t,cw_t,cm_t,lai_t)\n",
        "      tr_rho_s = Prosail2S2(filepath,tr_rho_s)\n",
        "      #plt.plot ( x, tr_rho_s, ':', label=\"Training prosail\")\n",
        "      #plt.legend(loc='best')\n",
        "      \n",
        "    if k > 1:\n",
        "      tr_rho_t = custom_prosail(cab_t,cw_t,cm_t,lai_t)\n",
        "      tr_rho_t = Prosail2S2(filepath,tr_rho_t)\n",
        "      tr_rho_s = np.vstack((tr_rho_s,tr_rho_t))\n",
        "      #plt.plot ( x, tr_rho_t, ':')\n",
        "\n",
        "    k = k+1\n",
        "\n",
        "\n",
        "  rho_samples=tr_rho_s\n",
        "\n",
        "\n",
        "  return rho_samples\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Function to store the outputs into a table\n",
        "column_names=[\"Model\",\n",
        "              \"NSamples\",\n",
        "              \"OutOfBag\",\n",
        "              \"KFold_tr_samples\",\n",
        "              \"KFold_vl_samples\",\n",
        "              \"Variable\",\n",
        "              \"Fold_nr\",\n",
        "              \"ExplVar\",\n",
        "              \"Max_err\",\n",
        "              \"Mean_abs_Err\",\n",
        "              \"Mean_sqr_err\",\n",
        "              #\"Mean_sqr_lg_err\",\n",
        "              \"Median_abs_err\",\n",
        "              \"r2\",\n",
        "              \"MAPE\",\n",
        "              \"ModelName\"]\n",
        "              #\"Mean_poiss_dev\",\n",
        "              #\"Mean_gamma_dev\"]\n",
        "              #\"Mean_tweed_dev\"]\n",
        "\n",
        "#mape is not existant in the package so we have to create it:\n",
        "#https://stats.stackexchange.com/questions/58391/mean-absolute-percentage-error-mape-in-scikit-learn\n",
        "#from sklearn.utils import check_array\n",
        "def mean_absolute_percentage_error(y_true, y_pred): \n",
        "\n",
        "    ## Note: does not handle mix 1d representation\n",
        "    #if _is_1d(y_true): \n",
        "    #    y_true, y_pred = _check_1d_array(y_true, y_pred)\n",
        "\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "  #Here, the file is used for saving\n",
        "  #creating a df to receive the data\n",
        "\n",
        "\n",
        "def calc_metrics(MDL,Samples,oob_samples,kf_tr,kf_vl,Variable,Fold,Ref,Pred,Modelname):\n",
        "\n",
        "  out_list = {\"Model\":MDL,\n",
        "              \"NSamples\":Samples,\n",
        "              \"OutOfBag\":oob_samples,\n",
        "              \"KFold_tr_samples\":kf_tr,\n",
        "              \"KFold_vl_samples\":kf_vl,\n",
        "              \"Variable\":Variable,\n",
        "              \"Fold_nr\":Fold,\n",
        "              \"ExplVar\": metrics.explained_variance_score(Ref, Pred),\n",
        "              \"Max_err\": metrics.max_error(Ref, Pred),\n",
        "              \"Mean_abs_Err\": metrics.mean_absolute_error(Ref, Pred),\n",
        "              \"Mean_sqr_err\": metrics.mean_squared_error(Ref, Pred),\n",
        "              \"Median_abs_err\" : metrics.median_absolute_error(Ref, Pred),\n",
        "              \"r2\": metrics.r2_score(Ref, Pred),\n",
        "              \"MAPE\": mean_absolute_percentage_error(Ref, Pred),\n",
        "              \"ModelName\":Modelname}\n",
        "\n",
        "\n",
        "  return out_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H02ce8EA1lW"
      },
      "source": [
        "# Step 4.2: Setting up machine learning models \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcaszIBQA94X"
      },
      "source": [
        "#Random forest and GPR\n",
        "from sklearn.gaussian_process.kernels import RBF,DotProduct,Matern,ExpSineSquared,RationalQuadratic\n",
        "#Random forest\n",
        "#initializing the random forest\n",
        "#optimized\n",
        "\n",
        "#{'min_samples_leaf': 0.0007218586390063501, 'min_samples_split': 0.0006729906646919075, 'n_estimators': 17} 800\n",
        "def rfr_call():\n",
        "\n",
        "  mdl = RandomForestRegressor(n_estimators=850,random_state=0,\n",
        "                               min_samples_leaf=0.0007218586390063501,\n",
        "                               min_samples_split=0.0006729906646919075,\n",
        "                               verbose=0) \n",
        "  return mdl\n",
        "\n",
        "rfr_ml = rfr_call()\n",
        "\n",
        "\n",
        "#gaussian processes with default acquisition function\n",
        "#initializing the gaussian process -optimized\n",
        "K_ratQ = RationalQuadratic(length_scale_bounds=\"fixed\",alpha_bounds=\"fixed\")\n",
        "\n",
        "#{'kernel': 1, 'n_restarts_optimizer': 7, 'normalize_y': 0}\n",
        "def gpr_call():\n",
        "  mdl = GaussianProcessRegressor(kernel=K_ratQ,\n",
        "                                  n_restarts_optimizer=80, # -> 1*RBF(1.0) is the default kerneºl\n",
        "                                  normalize_y=True,\n",
        "                                  random_state=0)\n",
        "  \n",
        "  return mdl\n",
        "\n",
        "gpr_ml = gpr_call()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAQglvU-eTKN"
      },
      "source": [
        "#parameters dictonary to build up the networks\n",
        "hiddenlayer = {0:'one',1:'two',2:'three'}\n",
        "modeltype = {0: 'MTST',1: 'MTSTST',2: 'MTMTST',3: 'MTMTSTST'}\n",
        "activation_f = {0:'linear', 1:'sigmoid', 2:'tanh',3:'exponential',4:'softplus',5:'relu',6:'softsign'}\n",
        "units_shared = {0: 5,1: 10,2: 15,3: 20}\n",
        "units_traits = {0:1,1:2,2:3,3:4,4:5,5:6,6:7,7:8,8:9,9:10}\n",
        "optimizer = {0: 'adam',1: 'RMSprop',2: 15,3: 'Adadelta'}\n",
        "\n",
        "#for the single task network\n",
        "param_dict_st_ann = {'aa_hiddenlayers':hiddenlayer[2],\n",
        "              'activation_3_1': activation_f[6],\n",
        "              'activation_3_2': activation_f[6],\n",
        "              'activation_3_3': activation_f[5],\n",
        "              'optimizer_3': optimizer[0],\n",
        "              'units_3_1': units_shared[3],\n",
        "              'units_3_2': units_shared[3],\n",
        "              'units_3_3': units_shared[2]}\n",
        "param_dict_st_ann\n",
        "#for the multi-task network\n",
        "param_dict_mtn = {'aaa_modeltype':modeltype[2],\n",
        "              'activation_MTMTST':activation_f[2],\n",
        "              'activation_MTMTST2': activation_f[5],\n",
        "              'activation_cab_MTMTST': activation_f[6],'activation_cm_MTMTST': activation_f[0],\n",
        "              'activation_cw_MTMTST': activation_f[6],'activation_lai_MTMTST': activation_f[2],\n",
        "              'optimizer_MTMTST': optimizer[0],\n",
        "              'units_cab_MTMTST': units_traits[7],'units_cm_MTMTST': units_traits[6],\n",
        "              'units_cw_MTMTST': units_traits[7],'units_lai_MTMTST': units_traits[8],\n",
        "              'units_shared_MTMTST': units_shared[3],\n",
        "              'units_shared_MTMTST2': units_shared[3]}\n",
        "\n",
        "\n",
        "#### setting up ANN\n",
        "def st_ann(params):\n",
        "\n",
        "  from keras.models import Sequential\n",
        "  from keras.layers.core import Dense, Activation\n",
        "  from keras.optimizers import Adadelta, Adam, rmsprop\n",
        "\n",
        "  mdl = Sequential()\n",
        "  #mdl.add(Dense(units=params['units_1'],input_dim=9,activation=params['activation_1']))\n",
        "  #mdl.add(Activation(params['activation_1']))\n",
        "  #print(\"This is the dictionary\")\n",
        "  #print(params)\n",
        "\n",
        "  hiddenlayers = units=params['aa_hiddenlayers']\n",
        "  \n",
        "  if hiddenlayers == 'one':\n",
        "    mdl.add(Dense(units=params['units_1_1'],input_dim=9,activation=params['activation_1_1']))\n",
        "\n",
        "    #and the output layer\n",
        "    mdl.add(Dense(4,activation='linear'))\n",
        "    #mdl.add(Activation(\"linear\"))\n",
        "    mdl.compile(loss='mean_squared_error', optimizer=params['optimizer_1'], metrics=['mean_squared_error'])\n",
        "  \n",
        "  if hiddenlayers == 'two':\n",
        "    mdl.add(Dense(units=params['units_2_1'],input_dim=9,activation=params['activation_2_1']))\n",
        "    mdl.add(Dense(units=params['units_2_2'],activation=params['activation_2_2']))\n",
        "\n",
        "    #and the output layer\n",
        "    mdl.add(Dense(4,activation='linear'))\n",
        "    #mdl.add(Activation(\"linear\"))\n",
        "    mdl.compile(loss='mean_squared_error', optimizer=params['optimizer_2'], metrics=['mean_squared_error'])\n",
        "    \n",
        "  if hiddenlayers == 'three':\n",
        "    mdl.add(Dense(units=params['units_3_1'],input_dim=9,activation=params['activation_3_1']))\n",
        "    mdl.add(Dense(units=params['units_3_2'],activation=params['activation_3_2']))\n",
        "    mdl.add(Dense(units=params['units_3_3'],activation=params['activation_3_3']))\n",
        "\n",
        "    #and the output layer\n",
        "    mdl.add(Dense(4,activation='linear'))\n",
        "    #mdl.add(Activation(\"linear\"))\n",
        "    mdl.compile(loss='mean_squared_error', optimizer=params['optimizer_3'], metrics=['mean_squared_error'])\n",
        "\n",
        "\n",
        "  return mdl\n",
        "\n",
        "\n",
        "#### setting up MTN\n",
        "def mt_ann(params):\n",
        "\n",
        "  #mdl = Sequential()\n",
        "  #mdl.add(Dense(units=params['units_1'],input_dim=9,activation=params['activation_1']))\n",
        "  #mdl.add(Activation(params['activation_1']))\n",
        "  #print(\"This is the dictionary\")\n",
        "  #print(params)\n",
        "\n",
        "  inputs = Input(shape=(9,))\n",
        "\n",
        "  #model selector\n",
        "  struct = params['aaa_modeltype']\n",
        "  if struct == 'MTST':\n",
        "    shared_MTST = Dense(units=params['units_shared_MTST'],activation=params['activation_MTST'])(inputs)\n",
        "    cab_MTST = Dense(units=params['units_cab_MTST'], activation=params['activation_cab_MTST'])(shared_MTST)\n",
        "    cw_MTST  = Dense(units=params['units_cw_MTST'],  activation=params['activation_cw_MTST'])(shared_MTST)\n",
        "    cm_MTST  = Dense(units=params['units_cm_MTST'],  activation=params['activation_cm_MTST'])(shared_MTST)\n",
        "    lai_MTST = Dense(units=params['units_lai_MTST'], activation=params['activation_lai_MTST'])(shared_MTST)\n",
        "\n",
        "    #this is a just a final output layer needed for the whole thing to work\n",
        "    cab_out = Dense(1, activation='linear')(cab_MTST)\n",
        "    cw_out  = Dense(1, activation='linear')(cw_MTST)\n",
        "    cm_out  = Dense(1, activation='linear')(cm_MTST)\n",
        "    lai_out = Dense(1, activation='linear')(lai_MTST)\n",
        "\n",
        "    mdl = Model(inputs=inputs, outputs=[cab_out,cw_out,cm_out,lai_out])\n",
        "    mdl.compile(loss='mean_squared_error', optimizer=params['optimizer_MTST'], metrics=['mean_squared_error'])\n",
        "\n",
        "\n",
        "  if struct == 'MTSTST':\n",
        "    shared_MTSTST = Dense(units=params['units_shared_MTSTST'],activation=params['activation_MTSTST'])(inputs)\n",
        "    cab_MTSTST = Dense(units=params['units_cab_MTSTST1'], activation=params['activation_cab_MTSTST1'])(shared_MTSTST)\n",
        "    cw_MTSTST  = Dense(units=params['units_cw_MTSTST1'],  activation=params['activation_cw_MTSTST1'])(shared_MTSTST)\n",
        "    cm_MTSTST  = Dense(units=params['units_cm_MTSTST1'],  activation=params['activation_cm_MTSTST1'])(shared_MTSTST)\n",
        "    lai_MTSTST = Dense(units=params['units_lai_MTSTST1'], activation=params['activation_lai_MTSTST1'])(shared_MTSTST)\n",
        "\n",
        "    cab_MTSTST2 = Dense(units=params['units_cab_MTSTST2'], activation=params['activation_cab_MTSTST2'])(cab_MTSTST)\n",
        "    cw_MTSTST2  = Dense(units=params['units_cw_MTSTST2'],  activation=params['activation_cw_MTSTST2'])(cw_MTSTST)\n",
        "    cm_MTSTST2  = Dense(units=params['units_cm_MTSTST2'],  activation=params['activation_cm_MTSTST2'])(cm_MTSTST)\n",
        "    lai_MTSTST2 = Dense(units=params['units_lai_MTSTST2'], activation=params['activation_lai_MTSTST2'])(lai_MTSTST)\n",
        "\n",
        "    #this is a just a final output layer needed for the whole thing to work\n",
        "    cab_out = Dense(1, activation='linear')(cab_MTSTST2)\n",
        "    cw_out  = Dense(1, activation='linear')(cw_MTSTST2)\n",
        "    cm_out  = Dense(1, activation='linear')(cm_MTSTST2)\n",
        "    lai_out = Dense(1, activation='linear')(lai_MTSTST2)\n",
        "\n",
        "    mdl = Model(inputs=inputs, outputs=[cab_out,cw_out,cm_out,lai_out])\n",
        "    mdl.compile(loss='mean_squared_error', optimizer=params['optimizer_MTSTST'], metrics=['mean_squared_error'])\n",
        "\n",
        "  if struct == 'MTMTST':\n",
        "    shared_MTMTST = Dense(units=params['units_shared_MTMTST'],activation=params['activation_MTMTST'])(inputs)\n",
        "    shared_MTMTST2 = Dense(units=params['units_shared_MTMTST2'],activation=params['activation_MTMTST2'])(shared_MTMTST)\n",
        "\n",
        "    cab_MTMTST = Dense(units=params['units_cab_MTMTST'], activation=params['activation_cab_MTMTST'])(shared_MTMTST2)\n",
        "    cw_MTMTST  = Dense(units=params['units_cw_MTMTST'],  activation=params['activation_cw_MTMTST'])(shared_MTMTST2)\n",
        "    cm_MTMTST  = Dense(units=params['units_cm_MTMTST'],  activation=params['activation_cm_MTMTST'])(shared_MTMTST2)\n",
        "    lai_MTMTST = Dense(units=params['units_lai_MTMTST'], activation=params['activation_lai_MTMTST'])(shared_MTMTST2)\n",
        "\n",
        "    #this is a just a final output layer needed for the whole thing to work\n",
        "    cab_out = Dense(1, activation='linear')(cab_MTMTST)\n",
        "    cw_out  = Dense(1, activation='linear')(cw_MTMTST)\n",
        "    cm_out  = Dense(1, activation='linear')(cm_MTMTST)\n",
        "    lai_out = Dense(1, activation='linear')(lai_MTMTST)\n",
        "\n",
        "    mdl = Model(inputs=inputs, outputs=[cab_out,cw_out,cm_out,lai_out])\n",
        "    mdl.compile(loss='mean_squared_error', optimizer=params['optimizer_MTMTST'], metrics=['mean_squared_error'])\n",
        "\n",
        "  if struct == 'MTMTSTST':\n",
        "    shared_MTMTSTST = Dense(units=params['units_shared_MTMTSTST'],activation=params['activation_MTMTSTST'])(inputs)\n",
        "    shared_MTMTSTST2 = Dense(units=params['units_shared_MTMTSTST2'],activation=params['activation_MTMTSTST2'])(shared_MTMTSTST)\n",
        "\n",
        "    cab_MTMTSTST = Dense(units=params['units_cab_MTMTSTST1'], activation=params['activation_cab_MTMTSTST1'])(shared_MTMTSTST2)\n",
        "    cw_MTMTSTST  = Dense(units=params['units_cw_MTMTSTST1'],  activation=params['activation_cw_MTMTSTST1'])(shared_MTMTSTST2)\n",
        "    cm_MTMTSTST  = Dense(units=params['units_cm_MTMTSTST1'],  activation=params['activation_cm_MTMTSTST1'])(shared_MTMTSTST2)\n",
        "    lai_MTMTSTST = Dense(units=params['units_lai_MTMTSTST1'], activation=params['activation_lai_MTMTSTST1'])(shared_MTMTSTST2)\n",
        "\n",
        "    cab_MTMTSTST2 = Dense(units=params['units_cab_MTMTSTST2'], activation=params['activation_cab_MTMTSTST2'])(cab_MTMTSTST)\n",
        "    cw_MTMTSTST2  = Dense(units=params['units_cw_MTMTSTST2'],  activation=params['activation_cw_MTMTSTST2'])(cw_MTMTSTST)\n",
        "    cm_MTMTSTST2  = Dense(units=params['units_cm_MTMTSTST2'],  activation=params['activation_cm_MTMTSTST2'])(cm_MTMTSTST)\n",
        "    lai_MTMTSTST2 = Dense(units=params['units_lai_MTMTSTST2'], activation=params['activation_lai_MTMTSTST2'])(lai_MTMTSTST)\n",
        "\n",
        "    #this is a just a final output layer needed for the whole thing to work\n",
        "    cab_out = Dense(1, activation='linear')(cab_MTMTSTST2)\n",
        "    cw_out  = Dense(1, activation='linear')(cw_MTMTSTST2)\n",
        "    cm_out  = Dense(1, activation='linear')(cm_MTMTSTST2)\n",
        "    lai_out = Dense(1, activation='linear')(lai_MTMTSTST2)\n",
        "\n",
        "    mdl = Model(inputs=inputs, outputs=[cab_out,cw_out,cm_out,lai_out])\n",
        "    mdl.compile(loss='mean_squared_error', optimizer=params['optimizer_MTMTSTST'], metrics=['mean_squared_error'])\n",
        "\n",
        "  return mdl\n",
        "\n",
        "\n",
        "#Training epochs\n",
        "number_epochs = 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4VwITmbAUA4"
      },
      "source": [
        "# Step 4.3: Trait-space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5QCX9diAhUE"
      },
      "source": [
        "n_traits=4 #I will test on 4 varying traits: cab, car, cw,cm,lai\n",
        "\n",
        "#generating a LHS hypercube (it uses a 0 to 1 interval that can be used as a multiplier against the different traits)\n",
        "np.random.seed(0)\n",
        "\n",
        "#the values here are 1 less than the max so i can add a value later to it\n",
        "#max_n=1 \n",
        "max_cab=121. #add 1\n",
        "#max_car=44. #add 1\n",
        "#max_cbrown= 9.99 #add 0.01\n",
        "max_cw=0.008 #add 0.001 \n",
        "max_cm=0.008 #0.001\n",
        "max_lai = 9.9 #add 0.1\n",
        "\n",
        "min_cab = 10.\n",
        "min_cw = 0.002\n",
        "min_cm = 0.002\n",
        "min_lai = .5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWFYoeGXAzWR"
      },
      "source": [
        "#step 5: setting up the Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKKf7l2OI4sW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a10cc37e-4f43-40b2-dad1-df6fc486b31b"
      },
      "source": [
        "#setting up the limits of the run\n",
        "\n",
        "#minimium samples\n",
        "n_samples = 100\n",
        "#n_points = np.arange(4000,n_samples-1,-250)\n",
        "#n_points= np.append(n_points,150)\n",
        "#n_points= np.append(n_points,50)\n",
        "#n_points = np.arange(3300,4000+1,100)\n",
        "#n_points = [50,100,150,200,250,300,400,500,600,700,800,900,1000,]\n",
        "#n_points = [50,150]\n",
        "#n_points = np.append(n_points,np.arange(n_samples,4000,250))\n",
        "#n_points= np.append(n_points,4000)\n",
        "#n_points= np.append(n_points,50)\n",
        "n_points = [50,100, 150, 200, 250, 350, 500, 750, 1000, 1250, 1500, 1750, 2000, 2500, 3000, 3500, 4000]\n",
        "n_points = [1750, 2000, 2500, 3000, 3500, 4000]\n",
        "\n",
        "print(\"Loop for the following set of LHS samples:\",n_points)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loop for the following set of LHS samples: [1750, 2000, 2500, 3000, 3500, 4000]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6uEBQZsJMoF"
      },
      "source": [
        "#step 6: The loop\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfx8nbF-JSCv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 961
        },
        "outputId": "d2e3627d-8486-4ff9-ccc0-53a6c261c7b8"
      },
      "source": [
        "### storing the dataset\n",
        "\n",
        "#######################################################\n",
        "#BEWARE - > Check the paths for file saving carefully\n",
        "########################################################\n",
        "\n",
        "#this is for the training dataset\n",
        "df_metrics = pd.DataFrame(columns=column_names)\n",
        "#this is for the out of bag\n",
        "df_metrics_valid = pd.DataFrame(columns=column_names)\n",
        "\n",
        "#a path to store the models \n",
        "#path2folder = \"/content/drive/My Drive/DSRP_Lunch_outputs/Models/\"\n",
        "path2folder = \"/content/drive/My Drive/RTM_Inversion/Pure/Run_02/Models\"\n",
        "\n",
        "#minimium samples\n",
        "#n_samples = 500\n",
        "#n_points = np.arange(n_samples,6000+1,500)\n",
        "\n",
        "print(\"Loop for the following set of LHS samples:\",n_points)\n",
        "\n",
        "#this command is enough to set u the k-fold\n",
        "fold_nr=5\n",
        "kf = KFold(n_splits=fold_nr,shuffle=True,random_state=0) # Define the split \n",
        "\n",
        "#this next comment snippet shouldn't be needed anymore\n",
        "\n",
        "##here we generate the out of bag test dataset of 1000 points that is used as out-of-bag testing\n",
        "#LHS_valid = lhsmdu.createRandomStandardUniformMatrix(n_traits,1000)\n",
        "#valid_pd_trait = pd.DataFrame.transpose(pd.DataFrame(LHS_valid))\n",
        "#valid_pd_trait.columns = [\"cab\",\"cw\",\"cm\",\"lai\"]\n",
        "\n",
        "#valid_pd_trait[\"cab\"]=valid_pd_trait[\"cab\"]*max_cab+min_cab\n",
        "#valid_pd_trait[\"cw\"] =valid_pd_trait[\"cw\"] *max_cw+min_cw\n",
        "#valid_pd_trait[\"cm\"] =valid_pd_trait[\"cm\"] *max_cm+min_cw\n",
        "#valid_pd_trait[\"lai\"]=valid_pd_trait[\"lai\"]*max_lai+min_lai\n",
        "\n",
        "#valid_np_trait = valid_pd_trait.iloc[:,:].values\n",
        "#valid_np_spect = Gen_spectra_data(valid_pd_trait)\n",
        "#valid_df_spect = valid_np_spect[:,[1,2,3,4,5,6,8,11,12]]\n",
        "\n",
        "for j in n_points:\n",
        "  print(\"Generating trait table for\",j,\"samples\")\n",
        "\n",
        "  #this section generates trait space - careful with the column names as this is significant later\n",
        "  LHS_train = lhsmdu.createRandomStandardUniformMatrix(n_traits,j)\n",
        "  pd_trait = pd.DataFrame.transpose(pd.DataFrame(LHS_train))\n",
        "  pd_trait.columns = [\"cab\",\"cw\",\"cm\",\"lai\"]\n",
        "\n",
        "  pd_trait[\"cab\"]=pd_trait[\"cab\"]*max_cab+min_cab\n",
        "  pd_trait[\"cw\"] =pd_trait[\"cw\"] *max_cw+min_cw\n",
        "  pd_trait[\"cm\"] =pd_trait[\"cm\"] *max_cm+min_cw\n",
        "  pd_trait[\"lai\"]=pd_trait[\"lai\"]*max_lai+min_lai\n",
        "\n",
        "  #lets also create a numpy object for the traits - this is for ease of operation later\n",
        "  np_trait = pd_trait.iloc[:,:].values\n",
        "\n",
        "  #until here its instantaneous\n",
        "\n",
        "  #now we first generate the date in hyperspectral, convolute it to S2 while iterating through the entire given trait space\n",
        "  np_spect = Gen_spectra_data(pd_trait)[:,[1,2,3,4,5,6,8,11,12]]\n",
        "  #trait_df = np_spectra\n",
        "\n",
        "  #now from these generated samples, we take out 10% of the points (aproximately, sometimes its a decimal more, depends on the number)\n",
        "  #creating row identifiers\n",
        "  #this gets the length for the entire list\n",
        "  index = list(range(len(np_spect)))\n",
        "\n",
        "  index10 = rdm.sample(index,math.ceil(len(index)*.1)) #randomly selects 10% of te data (aproximately)\n",
        "  index90 = [x for x in index if x not in index10] #makes a list set wit the reamining\n",
        "  #once we have the list of point, we can subset\n",
        "\n",
        "  vl_trait_df = np_trait[index10,]\n",
        "  tr_trait_df = np_trait[index90,]\n",
        "  \n",
        "  vl_spect_df = np_spect[index10,]\n",
        "  tr_spect_df = np_spect[index90,]\n",
        "\n",
        "  #this is a function to re-scale inputs for the neural networks\n",
        "  scaler = MinMaxScaler()\n",
        "\n",
        "  print(\"Of which, 10% were taken as out-of-bag:\", len(vl_trait_df))\n",
        "\n",
        "  #now we have to iterate per fold, train and store the prediction\n",
        "  k=1\n",
        "  for train_index, test_index in kf.split(tr_spect_df):\n",
        "\n",
        "    print(\"processing fold nr: \" + str(k))\n",
        "\n",
        "    #print(\"Calculating fold \",k,\"of\",fold_nr)\n",
        "    X_train, X_test = tr_spect_df[train_index], tr_spect_df[test_index]\n",
        "    Y_train, Y_test = tr_trait_df[train_index], tr_trait_df[test_index]\n",
        "    label_names = [\"cab\",\"cw\",\"cm\",\"lai\"]\n",
        "\n",
        "    #restarting the models\n",
        "    rfr_ml = rfr_call()\n",
        "    gpr_ml = gpr_call()\n",
        "    ann_ml = st_ann(param_dict_st_ann)\n",
        "    mtn_ml = mt_ann(param_dict_mtn)\n",
        "\n",
        "    #training the machine learning models\n",
        "    #random forest\n",
        "    rfr_ml.fit(X_train,Y_train)\n",
        "    #shallow multi objective single task neural network\n",
        "    scaler.fit(Y_train)\n",
        "    Y_train_norm = scaler.transform(Y_train)\n",
        "    ann_ml.fit(X_train,Y_train_norm,epochs=number_epochs,verbose=0)\n",
        "\n",
        "    #GPR - Training \n",
        "    gpr_ml.fit(X_train,Y_train)\n",
        "\n",
        "    #\"Shallow\" Multi task ANN training\n",
        "    cab_train = Y_train_norm[:,0]\n",
        "    cw_train = Y_train_norm[:,1]\n",
        "    cm_train = Y_train_norm[:,2]\n",
        "    lai_train = Y_train_norm[:,3]\n",
        "    mtn_ml.fit(X_train,[cab_train,cw_train,cm_train,lai_train],epochs=number_epochs,verbose=0)#hides spam on the output\n",
        "\n",
        "\n",
        "    #Prediction on training fold\n",
        "    y_ann = scaler.inverse_transform(ann_ml.predict(X_test))\n",
        "    y_rfr = rfr_ml.predict(X_test)\n",
        "    y_gpr = gpr_ml.predict(X_test)\n",
        "\n",
        "    #the multi task is a bit more complicated\n",
        "    Y_mtn = mtn_ml.predict(X_test)\n",
        "    cab_pred = Y_mtn[0]\n",
        "    cw_pred  = Y_mtn[1]\n",
        "    cm_pred  = Y_mtn[2]\n",
        "    lai_pred = Y_mtn[3]\n",
        "    np_Y_ann_pred = np.array(cab_pred)\n",
        "    np_Y_ann_pred = np.hstack((np_Y_ann_pred,cw_pred))\n",
        "    np_Y_ann_pred = np.hstack((np_Y_ann_pred,cm_pred))\n",
        "    np_Y_ann_pred = np.hstack((np_Y_ann_pred,lai_pred))\n",
        "\n",
        "    y_mtn = scaler.inverse_transform(np_Y_ann_pred)\n",
        "\n",
        "    #prediction on out of bag\n",
        "    y_ann_valid = scaler.inverse_transform(ann_ml.predict(vl_spect_df))\n",
        "    y_rfr_valid = rfr_ml.predict(vl_spect_df)\n",
        "    y_gpr_valid = gpr_ml.predict(vl_spect_df)\n",
        "\n",
        "    Y_mtn_valid = mtn_ml.predict(vl_spect_df)\n",
        "    cab_pred_valid = Y_mtn_valid[0]\n",
        "    cw_pred_valid  = Y_mtn_valid[1]\n",
        "    cm_pred_valid  = Y_mtn_valid[2]\n",
        "    lai_pred_valid = Y_mtn_valid[3]\n",
        "    np_Y_ann_pred_valid = np.array(cab_pred_valid)\n",
        "    np_Y_ann_pred_valid = np.hstack((np_Y_ann_pred_valid,cw_pred_valid))\n",
        "    np_Y_ann_pred_valid = np.hstack((np_Y_ann_pred_valid,cm_pred_valid))\n",
        "    np_Y_ann_pred_valid = np.hstack((np_Y_ann_pred_valid,lai_pred_valid))\n",
        "\n",
        "    y_mtn_valid = scaler.inverse_transform(np_Y_ann_pred_valid)\n",
        "\n",
        "\n",
        "    #setting the model filenames\n",
        "    #zfill is a handy function to standardize the output, i love it!\n",
        "    # rfr_ml_modelname = path2folder+\"RFR\"+\"_S\"+str(j).zfill(5)+\"_K\"+str(k).zfill(3)+\".pkl\"\n",
        "    # gpr_ml_modelname = path2folder+\"GPR\"+\"_S\"+str(j).zfill(5)+\"_K\"+str(k).zfill(3)+\".pkl\"\n",
        "    # ann_ml_modelname = path2folder+\"ANN\"+\"_S\"+str(j).zfill(5)+\"_K\"+str(k).zfill(3)+\".h5\"\n",
        "    # mtn_ml_modelname = path2folder+\"MTN\"+\"_S\"+str(j).zfill(5)+\"_K\"+str(k).zfill(3)+\".h5\"\n",
        "    rfr_ml_modelname = \"Not saved\"\n",
        "    gpr_ml_modelname = \"Not saved\"\n",
        "    ann_ml_modelname = \"Not saved\"\n",
        "    mtn_ml_modelname = \"Not saved\"\n",
        "\n",
        "\n",
        "    #COMMENT HERE IF YOU DO NOT WANT TO SAVE THE MODELS\n",
        "    #saving the models\n",
        "    #sklearn are pickles while keras are h5 files\n",
        "    #not saving random forest for now since i am pretty sure it wont be good but it is a big file...\n",
        "    #with open(rfr_ml_modelname, 'wb') as file:\n",
        "    #  pickle.dump(rfr_ml, file)\n",
        "\n",
        "    #with open(gpr_ml_modelname, 'wb') as file:\n",
        "    #  pickle.dump(gpr_ml, file)\n",
        "\n",
        "    #ann_ml.save(ann_ml_modelname)\n",
        "    #model.save(mtn_ml_modelname)\n",
        "      \n",
        "\n",
        "\n",
        "    #the next section stores all the results\n",
        "    for i in range(n_traits):\n",
        "\n",
        "      #this stores the training accuraccy metrics\n",
        "      #print(i)\n",
        "      ann_temp_list = calc_metrics(\"ANN\",j,len(vl_spect_df),len(X_train),len(X_test),label_names[i],k,Y_test[:,i],y_ann[:,i],ann_ml_modelname)\n",
        "      rfr_temp_list = calc_metrics(\"RFR\",j,len(vl_spect_df),len(X_train),len(X_test),label_names[i],k,Y_test[:,i],y_rfr[:,i],rfr_ml_modelname)\n",
        "      gpr_temp_list = calc_metrics(\"GPR\",j,len(vl_spect_df),len(X_train),len(X_test),label_names[i],k,Y_test[:,i],y_gpr[:,i],gpr_ml_modelname)\n",
        "      mtn_temp_list = calc_metrics(\"MTN\",j,len(vl_spect_df),len(X_train),len(X_test),label_names[i],k,Y_test[:,i],y_mtn[:,i],mtn_ml_modelname)\n",
        "\n",
        "      #This stores the metrics against the out of bag list\n",
        "      ann_temp_list_valid = calc_metrics(\"ANN\",j,len(vl_spect_df),len(X_train),len(X_test),label_names[i],k,vl_trait_df[:,i],y_ann_valid[:,i],ann_ml_modelname)\n",
        "      rfr_temp_list_valid = calc_metrics(\"RFR\",j,len(vl_spect_df),len(X_train),len(X_test),label_names[i],k,vl_trait_df[:,i],y_rfr_valid[:,i],rfr_ml_modelname)\n",
        "      gpr_temp_list_valid = calc_metrics(\"GPR\",j,len(vl_spect_df),len(X_train),len(X_test),label_names[i],k,vl_trait_df[:,i],y_gpr_valid[:,i],gpr_ml_modelname)\n",
        "      mtn_temp_list_valid = calc_metrics(\"MTN\",j,len(vl_spect_df),len(X_train),len(X_test),label_names[i],k,vl_trait_df[:,i],y_mtn_valid[:,i],mtn_ml_modelname)\n",
        "\n",
        "      #appending to the dataframe (training error)\n",
        "      df_metrics = df_metrics.append(ann_temp_list,ignore_index=True)\n",
        "      df_metrics = df_metrics.append(rfr_temp_list,ignore_index=True)\n",
        "      df_metrics = df_metrics.append(gpr_temp_list,ignore_index=True)\n",
        "      df_metrics = df_metrics.append(mtn_temp_list,ignore_index=True)\n",
        "\n",
        "      #appending to the dataframe (out of bag error)\n",
        "      df_metrics_valid = df_metrics_valid.append(ann_temp_list_valid,ignore_index=True)\n",
        "      df_metrics_valid = df_metrics_valid.append(rfr_temp_list_valid,ignore_index=True)\n",
        "      df_metrics_valid = df_metrics_valid.append(gpr_temp_list_valid,ignore_index=True)\n",
        "      df_metrics_valid = df_metrics_valid.append(mtn_temp_list_valid,ignore_index=True)\n",
        "    \n",
        "\n",
        "    k=k+1\n",
        "\n",
        "    #saves a temp version of the files\n",
        "    #saving the file to an output\n",
        "  \n",
        "  \n",
        "  #print(\"Of which\",len(X_train),\"were used for training and\",len(X_test),\"for validation\")\n",
        "  print(\"Saving temp files\")\n",
        "  df_metrics.to_csv(\"/content/drive/My Drive/RTM_Inversion/Pure/Run_02/Tables/Optim_Temp_KFold_TrainingError_csv2_Corrected_500epoch_1750.csv\",sep=\";\",decimal=\",\")\n",
        "  df_metrics_valid.to_csv(\"/content/drive/My Drive/RTM_Inversion/Pure/Run_02/Tables/Optim_Temp_KFold_OutOfBag_csv2_Corrected_500epoch_1750.csv\",sep=\";\",decimal=\",\")\n",
        "\n",
        "  df_metrics.to_csv(\"/content/drive/My Drive/RTM_Inversion/Pure/Run_02/Tables/Optim_Temp_KFold_TrainingError_csv1_Corrected_500epoch_1750.csv\",sep=\",\",decimal=\".\")\n",
        "  df_metrics_valid.to_csv(\"/content/drive/My Drive/RTM_Inversion/Pure/Run_02/Tables/Optim_Temp_KFold_OutOfBag_csv1_Corrected_500epoch_1750.csv\",sep=\",\",decimal=\".\")\n",
        "\n",
        "\n",
        "#saving the file to an output\n",
        "df_metrics.to_csv(\"/content/drive/My Drive/RTM_Inversion/Pure/Run_02/Tables/Optim_KFold_TrainingError_csv2_Corrected_500epoch_1750.csv\",sep=\";\",decimal=\",\")\n",
        "df_metrics_valid.to_csv(\"/content/drive/My Drive/RTM_Inversion/Pure/Run_02/Tables/Optim_KFold_OutOfBag_csv2_Corrected_500epoch_1750.csv\",sep=\";\",decimal=\",\")\n",
        "\n",
        "df_metrics.to_csv(\"/content/drive/My Drive/RTM_Inversion/Pure/Run_02/Tables/Optim_KFold_TrainingError_csv1_Corrected_500epoch_1750.csv\",sep=\",\",decimal=\".\")\n",
        "df_metrics_valid.to_csv(\"/content/drive/My Drive/RTM_Inversion/Pure/Run_02/Tables/Optim_KFold_OutOfBag_csv1_Corrected_500epoch_1750.csv\",sep=\",\",decimal=\".\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loop for the following set of LHS samples: [1750, 2000, 2500, 3000, 3500, 4000]\n",
            "Generating trait table for 1750 samples\n",
            "Of which, 10% were taken as out-of-bag: 175\n",
            "processing fold nr: 1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "processing fold nr: 2\n",
            "processing fold nr: 3\n",
            "processing fold nr: 4\n",
            "processing fold nr: 5\n",
            "Saving temp files\n",
            "Generating trait table for 2000 samples\n",
            "Of which, 10% were taken as out-of-bag: 200\n",
            "processing fold nr: 1\n",
            "processing fold nr: 2\n",
            "processing fold nr: 3\n",
            "processing fold nr: 4\n",
            "processing fold nr: 5\n",
            "Saving temp files\n",
            "Generating trait table for 2500 samples\n",
            "Of which, 10% were taken as out-of-bag: 250\n",
            "processing fold nr: 1\n",
            "processing fold nr: 2\n",
            "processing fold nr: 3\n",
            "processing fold nr: 4\n",
            "processing fold nr: 5\n",
            "Saving temp files\n",
            "Generating trait table for 3000 samples\n",
            "Of which, 10% were taken as out-of-bag: 300\n",
            "processing fold nr: 1\n",
            "processing fold nr: 2\n",
            "processing fold nr: 3\n",
            "processing fold nr: 4\n",
            "processing fold nr: 5\n",
            "Saving temp files\n",
            "Generating trait table for 3500 samples\n",
            "Of which, 10% were taken as out-of-bag: 350\n",
            "processing fold nr: 1\n",
            "processing fold nr: 2\n",
            "processing fold nr: 3\n",
            "processing fold nr: 4\n",
            "processing fold nr: 5\n",
            "Saving temp files\n",
            "Generating trait table for 4000 samples\n",
            "Of which, 10% were taken as out-of-bag: 400\n",
            "processing fold nr: 1\n",
            "processing fold nr: 2\n",
            "processing fold nr: 3\n",
            "processing fold nr: 4\n",
            "processing fold nr: 5\n",
            "Saving temp files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhU1J8UyKq54"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}